{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjuDH+0NfGzKoDparmEtCr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wai-ming-chan/fed_avg/blob/main/sanityCheck_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo program to train MNIST with Pytorch (Centralized training)\n",
        "\n",
        "Reference: [https://meinkappa.github.io/blog/2021/09/16/MNIST-In-Pytorch.html](https://meinkappa.github.io/blog/2021/09/16/MNIST-In-Pytorch.html)"
      ],
      "metadata": {
        "id": "Mb6i00YcPZuK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1oMz_Z7RPJUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06274567-ebff-4a0c-f4e5-ea87549c2876"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "# libraries for pyTorch model Visualization\n",
        "! pip install -q torchview\n",
        "! pip install -q -U graphviz\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torchview import draw_graph\n",
        "from torch import nn\n",
        "import torch\n",
        "import graphviz\n",
        "\n",
        "# when running on VSCode run the below command\n",
        "# svg format on vscode does not give desired result\n",
        "graphviz.set_jupyter_format('png')\n",
        "#-----------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading dataset\n"
      ],
      "metadata": {
        "id": "rVA-E840a8nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grab MNIST data with torchvision datasets\n",
        "## We can tell Pytorch how to manipulate the dataset by giving details.\n",
        "##\n",
        "### root: Where to store the data. We are storing it in data directory.\n",
        "### train: Whether to grab training dataset or testing dataset. \n",
        "###         Given True value, training_data is a training dataset from MNIST. \n",
        "###         On the other hand, test_data is a testing dataset from MNIST.\n",
        "### download: Whether to download if data is not already in root. We passed True to download the dataset.\n",
        "### transform: What to do with data. We are converting our images of handwritten digits into Pytorch tensors so that we can train our model.\n",
        "\n",
        "training_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# Set GPU or CPU device. If GPU is available, we use it to speed up the training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('device: ', device)\n",
        "\n",
        "# check the dimension of data\n",
        "print(training_data)\n",
        "print('training data size: ', len(training_data) )\n",
        "print('test data size: ', len(test_data) )\n",
        "print(training_data[0][0].shape)\n",
        "print(training_data[0][0].squeeze().shape)\n",
        "# plt.imshow(training_data[len(training_data)-1][0].squeeze(), cmap=\"gray\");\n",
        "# print('label: ', training_data[len(training_data)-1][1])\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(10,10))\n",
        "cols, rows = 4, 2\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(label)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "fPbRQJU1QAsg",
        "outputId": "650d7abe-7282-49a3-9e62-546c2bdd744e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cpu\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "training data size:  60000\n",
            "test data size:  10000\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataloader. \n",
        "## A dataloader divides our data by a given batch_size and hands out each one to our model for training\n",
        "## Our train_dataloader will have 64 images per batch, which makes a total of 157 batches.\n",
        "bs=64\n",
        "train_dataloader = DataLoader(training_data, batch_size=bs)\n",
        "test_dataloader = DataLoader(test_data, batch_size=bs)\n",
        "\n",
        "print(10000/64, len(test_dataloader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7gGRZDnVpPn",
        "outputId": "a5161cb2-36ab-4b62-8f4f-cb6feb9f5c9d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156.25 157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup our model now.\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# Feed our model into GPU now if there is one.\n",
        "model = NeuralNetwork().to(device)\n",
        "# print(model)\n",
        "\n",
        "from torchsummary import summary\n",
        "help(summary)\n",
        "# summary(model, input_size=(1,784), batch_size=bs)\n",
        "\n",
        "\n",
        "# Model 1: 2NN\n",
        "class MNIST_2NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model_2NN = MNIST_2NN().to(device)\n",
        "summary(model_2NN, input_size=(1,784), batch_size=bs)\n",
        "\n",
        "\n",
        "model_graph = draw_graph(\n",
        "  model_2NN, \n",
        "  input_size=(bs,784), \n",
        "  graph_name='my2NN',\n",
        "  hide_inner_tensors=True,\n",
        "  hide_module_functions=False,\n",
        "  expand_nested=True,\n",
        "  roll=True, # rolls recursive models\n",
        "  save_graph=True\n",
        ")\n",
        "model_graph.visual_graph\n",
        "#-----------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xFw7ceI7ax8l",
        "outputId": "f3a5e116-e4ce-4bb5-aac2-c48d3641d85b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Help on function summary in module torchsummary.torchsummary:\n",
            "\n",
            "summary(model, input_size, batch_size=-1, device='cuda')\n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                  [64, 784]               0\n",
            "            Linear-2                  [64, 512]         401,920\n",
            "              ReLU-3                  [64, 512]               0\n",
            "            Linear-4                   [64, 10]           5,130\n",
            "================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 0.89\n",
            "Params size (MB): 1.55\n",
            "Estimated Total Size (MB): 2.63\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                  [64, 784]               0\n",
            "            Linear-2                  [64, 200]         157,000\n",
            "              ReLU-3                  [64, 200]               0\n",
            "            Linear-4                  [64, 200]          40,200\n",
            "              ReLU-5                  [64, 200]               0\n",
            "            Linear-6                   [64, 10]           2,010\n",
            "================================================================\n",
            "Total params: 199,210\n",
            "Trainable params: 199,210\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 0.78\n",
            "Params size (MB): 0.76\n",
            "Estimated Total Size (MB): 1.73\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7f136a527310>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of our defined models"
      ],
      "metadata": {
        "id": "AzC4m8_faVP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  # Visualization of model and trace (method 1: Torchviz )\n",
        "  ## ref: https://github.com/szagoruyko/pytorchviz\n",
        "  !pip install torchviz\n",
        "  from torchviz import make_dot\n",
        "\n",
        "  # model = nn.Sequential()\n",
        "  # model.add_module('W0', nn.Linear(8, 16))\n",
        "  # model.add_module('tanh', nn.Tanh())\n",
        "  # model.add_module('W1', nn.Linear(16, 1))\n",
        "  # make_dot(y.mean(), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)\n",
        "\n",
        "  # x = torch.randn(1, 8)\n",
        "  # y = model(x)\n",
        "  # print('x: ', x)\n",
        "  # print('y: ', y)\n",
        "\n",
        "  model = NeuralNetwork().to(device)\n",
        "  sample_idx = torch.randint(len(training_data), size=(1,)).item() # randomly pick one sample\n",
        "  img, label = training_data[sample_idx]\n",
        "  y = model(img)\n",
        "  make_dot(y.mean(), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)\n"
      ],
      "metadata": {
        "id": "3x6lqgb7bEdf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  # Visualization of pyTorch model (method 2: torchview)\n",
        "  # source: https://github.com/mert-kurttutan/torchview\n",
        "  ! pip install -q torchview\n",
        "  ! pip install -q -U graphviz\n",
        "\n",
        "  import torchvision\n",
        "\n",
        "  from torchview import draw_graph\n",
        "  from torch import nn\n",
        "  import torch\n",
        "  import graphviz\n",
        "\n",
        "  # when running on VSCode run the below command\n",
        "  # svg format on vscode does not give desired result\n",
        "  graphviz.set_jupyter_format('png')\n",
        "\n",
        "  summary(model, input_size=(1,784), batch_size=64)\n",
        "\n",
        "  batch_size=64\n",
        "  model_graph = draw_graph(\n",
        "    model, \n",
        "    input_size=(batch_size,784), \n",
        "    graph_name='myNN',\n",
        "    hide_inner_tensors=True,\n",
        "    hide_module_functions=False,\n",
        "    expand_nested=True,\n",
        "    roll=True, # rolls recursive models\n",
        "    save_graph=True\n",
        "  )\n",
        "  model_graph.visual_graph\n"
      ],
      "metadata": {
        "id": "zLyKLaivn9Vy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some model hyperparameters setting"
      ],
      "metadata": {
        "id": "LfoM0sTYanZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model parameters setting (will affect performance of the model)\n",
        "lr = 1e-3     # learning rate\n",
        "bs = 64       # batch size\n",
        "epochs = 5    # more epochs the more likely our model is going to learn from the dataset. However, too many epochs will overfit our model\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # Classificatin problem, we use Cross Entropy as loss function\n",
        "\n",
        "optimizer = torch.optim.SGD(model_2NN.parameters(), lr=lr) # an optimizer to update our parameters, we use stochastic gradient descent to reduce the loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "a3zrsL8Ca4Wy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define how we train our data\n",
        "def train_data(model):\n",
        "    for xb, yb in train_dataloader:   # fetch xb (a batch of images) and yb (a batch of labels) from train_dataloader\n",
        "        preds = model(xb)             # make a prediction\n",
        "        loss = loss_fn(preds, yb)     # compute the loss value\n",
        "        optimizer.zero_grad()         # set our gradient to zero\n",
        "        loss.backward()               # gradient descent\n",
        "        optimizer.step()              # update our weights and biases\n",
        "    loss = loss.item()\n",
        "    print(f\"Train loss: {loss:>7f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define how we test our model\n",
        "def test_data(model):\n",
        "    num_batches = len(test_dataloader)\n",
        "    size = len(test_dataloader.dataset)\n",
        "    test_loss, corrects = 0, 0\n",
        "\n",
        "    with torch.no_grad():             # Disabling gradient calculation for reduction of memory consumption for computations\n",
        "        for xb, yb in test_dataloader:\n",
        "            preds = model(xb)\n",
        "            test_loss += loss_fn(preds, yb).item()\n",
        "            corrects += (preds.argmax(1) == yb).type(torch.float).sum().item()\n",
        "    \n",
        "    test_loss /= num_batches\n",
        "    # test_loss = lo\n",
        "    corrects /= size\n",
        "    print(f\"Test loss: \\n Accuracy: {(100*corrects):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "\n",
        "for t in range(1000):\n",
        "  print('epoch: ', t)\n",
        "  train_data(model_2NN)\n",
        "  test_data(model_2NN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S2YHlCTWceUS",
        "outputId": "1acccdf1-a8b7-4c57-dab8-7f5254287e35"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "Train loss: 0.254169\n",
            "Test loss: \n",
            " Accuracy: 89.0%, Avg loss: 0.393770 \n",
            "\n",
            "epoch:  1\n",
            "Train loss: 0.243668\n",
            "Test loss: \n",
            " Accuracy: 89.1%, Avg loss: 0.384797 \n",
            "\n",
            "epoch:  2\n",
            "Train loss: 0.234236\n",
            "Test loss: \n",
            " Accuracy: 89.4%, Avg loss: 0.376841 \n",
            "\n",
            "epoch:  3\n",
            "Train loss: 0.225685\n",
            "Test loss: \n",
            " Accuracy: 89.4%, Avg loss: 0.369722 \n",
            "\n",
            "epoch:  4\n",
            "Train loss: 0.217895\n",
            "Test loss: \n",
            " Accuracy: 89.6%, Avg loss: 0.363298 \n",
            "\n",
            "epoch:  5\n",
            "Train loss: 0.210764\n",
            "Test loss: \n",
            " Accuracy: 89.8%, Avg loss: 0.357460 \n",
            "\n",
            "epoch:  6\n",
            "Train loss: 0.204199\n",
            "Test loss: \n",
            " Accuracy: 90.0%, Avg loss: 0.352107 \n",
            "\n",
            "epoch:  7\n",
            "Train loss: 0.198073\n",
            "Test loss: \n",
            " Accuracy: 90.1%, Avg loss: 0.347172 \n",
            "\n",
            "epoch:  8\n",
            "Train loss: 0.192319\n",
            "Test loss: \n",
            " Accuracy: 90.3%, Avg loss: 0.342598 \n",
            "\n",
            "epoch:  9\n",
            "Train loss: 0.186886\n",
            "Test loss: \n",
            " Accuracy: 90.3%, Avg loss: 0.338328 \n",
            "\n",
            "epoch:  10\n",
            "Train loss: 0.181833\n",
            "Test loss: \n",
            " Accuracy: 90.4%, Avg loss: 0.334325 \n",
            "\n",
            "epoch:  11\n",
            "Train loss: 0.177157\n",
            "Test loss: \n",
            " Accuracy: 90.5%, Avg loss: 0.330563 \n",
            "\n",
            "epoch:  12\n",
            "Train loss: 0.172725\n",
            "Test loss: \n",
            " Accuracy: 90.6%, Avg loss: 0.327003 \n",
            "\n",
            "epoch:  13\n",
            "Train loss: 0.168522\n",
            "Test loss: \n",
            " Accuracy: 90.7%, Avg loss: 0.323630 \n",
            "\n",
            "epoch:  14\n",
            "Train loss: 0.164518\n",
            "Test loss: \n",
            " Accuracy: 90.8%, Avg loss: 0.320423 \n",
            "\n",
            "epoch:  15\n",
            "Train loss: 0.160729\n",
            "Test loss: \n",
            " Accuracy: 90.9%, Avg loss: 0.317359 \n",
            "\n",
            "epoch:  16\n",
            "Train loss: 0.157131\n",
            "Test loss: \n",
            " Accuracy: 91.0%, Avg loss: 0.314427 \n",
            "\n",
            "epoch:  17\n",
            "Train loss: 0.153695\n",
            "Test loss: \n",
            " Accuracy: 91.0%, Avg loss: 0.311610 \n",
            "\n",
            "epoch:  18\n",
            "Train loss: 0.150434\n",
            "Test loss: \n",
            " Accuracy: 91.1%, Avg loss: 0.308903 \n",
            "\n",
            "epoch:  19\n",
            "Train loss: 0.147353\n",
            "Test loss: \n",
            " Accuracy: 91.2%, Avg loss: 0.306297 \n",
            "\n",
            "epoch:  20\n",
            "Train loss: 0.144397\n",
            "Test loss: \n",
            " Accuracy: 91.3%, Avg loss: 0.303776 \n",
            "\n",
            "epoch:  21\n",
            "Train loss: 0.141548\n",
            "Test loss: \n",
            " Accuracy: 91.3%, Avg loss: 0.301332 \n",
            "\n",
            "epoch:  22\n",
            "Train loss: 0.138863\n",
            "Test loss: \n",
            " Accuracy: 91.3%, Avg loss: 0.298971 \n",
            "\n",
            "epoch:  23\n",
            "Train loss: 0.136297\n",
            "Test loss: \n",
            " Accuracy: 91.4%, Avg loss: 0.296681 \n",
            "\n",
            "epoch:  24\n",
            "Train loss: 0.133829\n",
            "Test loss: \n",
            " Accuracy: 91.4%, Avg loss: 0.294465 \n",
            "\n",
            "epoch:  25\n",
            "Train loss: 0.131593\n",
            "Test loss: \n",
            " Accuracy: 91.5%, Avg loss: 0.292313 \n",
            "\n",
            "epoch:  26\n",
            "Train loss: 0.129410\n",
            "Test loss: \n",
            " Accuracy: 91.6%, Avg loss: 0.290210 \n",
            "\n",
            "epoch:  27\n",
            "Train loss: 0.127320\n",
            "Test loss: \n",
            " Accuracy: 91.7%, Avg loss: 0.288161 \n",
            "\n",
            "epoch:  28\n",
            "Train loss: 0.125311\n",
            "Test loss: \n",
            " Accuracy: 91.8%, Avg loss: 0.286163 \n",
            "\n",
            "epoch:  29\n",
            "Train loss: 0.123391\n",
            "Test loss: \n",
            " Accuracy: 91.8%, Avg loss: 0.284205 \n",
            "\n",
            "epoch:  30\n",
            "Train loss: 0.121569\n",
            "Test loss: \n",
            " Accuracy: 91.9%, Avg loss: 0.282289 \n",
            "\n",
            "epoch:  31\n",
            "Train loss: 0.119736\n",
            "Test loss: \n",
            " Accuracy: 92.0%, Avg loss: 0.280403 \n",
            "\n",
            "epoch:  32\n",
            "Train loss: 0.117954\n",
            "Test loss: \n",
            " Accuracy: 92.0%, Avg loss: 0.278558 \n",
            "\n",
            "epoch:  33\n",
            "Train loss: 0.116229\n",
            "Test loss: \n",
            " Accuracy: 92.1%, Avg loss: 0.276748 \n",
            "\n",
            "epoch:  34\n",
            "Train loss: 0.114534\n",
            "Test loss: \n",
            " Accuracy: 92.1%, Avg loss: 0.274965 \n",
            "\n",
            "epoch:  35\n",
            "Train loss: 0.112850\n",
            "Test loss: \n",
            " Accuracy: 92.2%, Avg loss: 0.273209 \n",
            "\n",
            "epoch:  36\n",
            "Train loss: 0.111250\n",
            "Test loss: \n",
            " Accuracy: 92.2%, Avg loss: 0.271484 \n",
            "\n",
            "epoch:  37\n",
            "Train loss: 0.109690\n",
            "Test loss: \n",
            " Accuracy: 92.2%, Avg loss: 0.269783 \n",
            "\n",
            "epoch:  38\n",
            "Train loss: 0.108198\n",
            "Test loss: \n",
            " Accuracy: 92.2%, Avg loss: 0.268121 \n",
            "\n",
            "epoch:  39\n",
            "Train loss: 0.106750\n",
            "Test loss: \n",
            " Accuracy: 92.2%, Avg loss: 0.266487 \n",
            "\n",
            "epoch:  40\n",
            "Train loss: 0.105365\n",
            "Test loss: \n",
            " Accuracy: 92.3%, Avg loss: 0.264878 \n",
            "\n",
            "epoch:  41\n",
            "Train loss: 0.104011\n",
            "Test loss: \n",
            " Accuracy: 92.4%, Avg loss: 0.263283 \n",
            "\n",
            "epoch:  42\n",
            "Train loss: 0.102700\n",
            "Test loss: \n",
            " Accuracy: 92.5%, Avg loss: 0.261711 \n",
            "\n",
            "epoch:  43\n",
            "Train loss: 0.101389\n",
            "Test loss: \n",
            " Accuracy: 92.5%, Avg loss: 0.260153 \n",
            "\n",
            "epoch:  44\n",
            "Train loss: 0.100125\n",
            "Test loss: \n",
            " Accuracy: 92.6%, Avg loss: 0.258614 \n",
            "\n",
            "epoch:  45\n",
            "Train loss: 0.098863\n",
            "Test loss: \n",
            " Accuracy: 92.6%, Avg loss: 0.257095 \n",
            "\n",
            "epoch:  46\n",
            "Train loss: 0.097618\n",
            "Test loss: \n",
            " Accuracy: 92.6%, Avg loss: 0.255591 \n",
            "\n",
            "epoch:  47\n",
            "Train loss: 0.096373\n",
            "Test loss: \n",
            " Accuracy: 92.7%, Avg loss: 0.254101 \n",
            "\n",
            "epoch:  48\n",
            "Train loss: 0.095161\n",
            "Test loss: \n",
            " Accuracy: 92.7%, Avg loss: 0.252634 \n",
            "\n",
            "epoch:  49\n",
            "Train loss: 0.094045\n",
            "Test loss: \n",
            " Accuracy: 92.8%, Avg loss: 0.251181 \n",
            "\n",
            "epoch:  50\n",
            "Train loss: 0.092928\n",
            "Test loss: \n",
            " Accuracy: 92.8%, Avg loss: 0.249743 \n",
            "\n",
            "epoch:  51\n",
            "Train loss: 0.091818\n",
            "Test loss: \n",
            " Accuracy: 92.9%, Avg loss: 0.248317 \n",
            "\n",
            "epoch:  52\n",
            "Train loss: 0.090784\n",
            "Test loss: \n",
            " Accuracy: 93.0%, Avg loss: 0.246903 \n",
            "\n",
            "epoch:  53\n",
            "Train loss: 0.089808\n",
            "Test loss: \n",
            " Accuracy: 93.0%, Avg loss: 0.245499 \n",
            "\n",
            "epoch:  54\n",
            "Train loss: 0.088847\n",
            "Test loss: \n",
            " Accuracy: 93.1%, Avg loss: 0.244104 \n",
            "\n",
            "epoch:  55\n",
            "Train loss: 0.087885\n",
            "Test loss: \n",
            " Accuracy: 93.2%, Avg loss: 0.242715 \n",
            "\n",
            "epoch:  56\n",
            "Train loss: 0.086941\n",
            "Test loss: \n",
            " Accuracy: 93.2%, Avg loss: 0.241341 \n",
            "\n",
            "epoch:  57\n",
            "Train loss: 0.085960\n",
            "Test loss: \n",
            " Accuracy: 93.3%, Avg loss: 0.239978 \n",
            "\n",
            "epoch:  58\n",
            "Train loss: 0.084992\n",
            "Test loss: \n",
            " Accuracy: 93.3%, Avg loss: 0.238619 \n",
            "\n",
            "epoch:  59\n",
            "Train loss: 0.084031\n",
            "Test loss: \n",
            " Accuracy: 93.4%, Avg loss: 0.237271 \n",
            "\n",
            "epoch:  60\n",
            "Train loss: 0.083024\n",
            "Test loss: \n",
            " Accuracy: 93.4%, Avg loss: 0.235932 \n",
            "\n",
            "epoch:  61\n",
            "Train loss: 0.082065\n",
            "Test loss: \n",
            " Accuracy: 93.5%, Avg loss: 0.234600 \n",
            "\n",
            "epoch:  62\n",
            "Train loss: 0.081128\n",
            "Test loss: \n",
            " Accuracy: 93.5%, Avg loss: 0.233279 \n",
            "\n",
            "epoch:  63\n",
            "Train loss: 0.080248\n",
            "Test loss: \n",
            " Accuracy: 93.5%, Avg loss: 0.231970 \n",
            "\n",
            "epoch:  64\n",
            "Train loss: 0.079342\n",
            "Test loss: \n",
            " Accuracy: 93.6%, Avg loss: 0.230670 \n",
            "\n",
            "epoch:  65\n",
            "Train loss: 0.078445\n",
            "Test loss: \n",
            " Accuracy: 93.6%, Avg loss: 0.229374 \n",
            "\n",
            "epoch:  66\n",
            "Train loss: 0.077592\n",
            "Test loss: \n",
            " Accuracy: 93.7%, Avg loss: 0.228098 \n",
            "\n",
            "epoch:  67\n",
            "Train loss: 0.076757\n",
            "Test loss: \n",
            " Accuracy: 93.7%, Avg loss: 0.226827 \n",
            "\n",
            "epoch:  68\n",
            "Train loss: 0.075913\n",
            "Test loss: \n",
            " Accuracy: 93.7%, Avg loss: 0.225570 \n",
            "\n",
            "epoch:  69\n",
            "Train loss: 0.075101\n",
            "Test loss: \n",
            " Accuracy: 93.8%, Avg loss: 0.224327 \n",
            "\n",
            "epoch:  70\n",
            "Train loss: 0.074320\n",
            "Test loss: \n",
            " Accuracy: 93.8%, Avg loss: 0.223091 \n",
            "\n",
            "epoch:  71\n",
            "Train loss: 0.073544\n",
            "Test loss: \n",
            " Accuracy: 93.9%, Avg loss: 0.221861 \n",
            "\n",
            "epoch:  72\n",
            "Train loss: 0.072789\n",
            "Test loss: \n",
            " Accuracy: 93.9%, Avg loss: 0.220640 \n",
            "\n",
            "epoch:  73\n",
            "Train loss: 0.072066\n",
            "Test loss: \n",
            " Accuracy: 93.9%, Avg loss: 0.219422 \n",
            "\n",
            "epoch:  74\n",
            "Train loss: 0.071349\n",
            "Test loss: \n",
            " Accuracy: 94.0%, Avg loss: 0.218215 \n",
            "\n",
            "epoch:  75\n",
            "Train loss: 0.070648\n",
            "Test loss: \n",
            " Accuracy: 94.0%, Avg loss: 0.217015 \n",
            "\n",
            "epoch:  76\n",
            "Train loss: 0.069957\n",
            "Test loss: \n",
            " Accuracy: 94.0%, Avg loss: 0.215822 \n",
            "\n",
            "epoch:  77\n",
            "Train loss: 0.069282\n",
            "Test loss: \n",
            " Accuracy: 94.0%, Avg loss: 0.214643 \n",
            "\n",
            "epoch:  78\n",
            "Train loss: 0.068636\n",
            "Test loss: \n",
            " Accuracy: 94.0%, Avg loss: 0.213477 \n",
            "\n",
            "epoch:  79\n",
            "Train loss: 0.067972\n",
            "Test loss: \n",
            " Accuracy: 94.1%, Avg loss: 0.212318 \n",
            "\n",
            "epoch:  80\n",
            "Train loss: 0.067317\n",
            "Test loss: \n",
            " Accuracy: 94.1%, Avg loss: 0.211175 \n",
            "\n",
            "epoch:  81\n",
            "Train loss: 0.066720\n",
            "Test loss: \n",
            " Accuracy: 94.1%, Avg loss: 0.210047 \n",
            "\n",
            "epoch:  82\n",
            "Train loss: 0.066122\n",
            "Test loss: \n",
            " Accuracy: 94.2%, Avg loss: 0.208926 \n",
            "\n",
            "epoch:  83\n",
            "Train loss: 0.065543\n",
            "Test loss: \n",
            " Accuracy: 94.2%, Avg loss: 0.207816 \n",
            "\n",
            "epoch:  84\n",
            "Train loss: 0.064981\n",
            "Test loss: \n",
            " Accuracy: 94.2%, Avg loss: 0.206711 \n",
            "\n",
            "epoch:  85\n",
            "Train loss: 0.064460\n",
            "Test loss: \n",
            " Accuracy: 94.2%, Avg loss: 0.205615 \n",
            "\n",
            "epoch:  86\n",
            "Train loss: 0.063955\n",
            "Test loss: \n",
            " Accuracy: 94.3%, Avg loss: 0.204527 \n",
            "\n",
            "epoch:  87\n",
            "Train loss: 0.063473\n",
            "Test loss: \n",
            " Accuracy: 94.3%, Avg loss: 0.203450 \n",
            "\n",
            "epoch:  88\n",
            "Train loss: 0.063001\n",
            "Test loss: \n",
            " Accuracy: 94.3%, Avg loss: 0.202380 \n",
            "\n",
            "epoch:  89\n",
            "Train loss: 0.062529\n",
            "Test loss: \n",
            " Accuracy: 94.3%, Avg loss: 0.201322 \n",
            "\n",
            "epoch:  90\n",
            "Train loss: 0.062079\n",
            "Test loss: \n",
            " Accuracy: 94.3%, Avg loss: 0.200272 \n",
            "\n",
            "epoch:  91\n",
            "Train loss: 0.061644\n",
            "Test loss: \n",
            " Accuracy: 94.4%, Avg loss: 0.199234 \n",
            "\n",
            "epoch:  92\n",
            "Train loss: 0.061226\n",
            "Test loss: \n",
            " Accuracy: 94.4%, Avg loss: 0.198201 \n",
            "\n",
            "epoch:  93\n",
            "Train loss: 0.060832\n",
            "Test loss: \n",
            " Accuracy: 94.4%, Avg loss: 0.197182 \n",
            "\n",
            "epoch:  94\n",
            "Train loss: 0.060453\n",
            "Test loss: \n",
            " Accuracy: 94.5%, Avg loss: 0.196164 \n",
            "\n",
            "epoch:  95\n",
            "Train loss: 0.060062\n",
            "Test loss: \n",
            " Accuracy: 94.5%, Avg loss: 0.195160 \n",
            "\n",
            "epoch:  96\n",
            "Train loss: 0.059648\n",
            "Test loss: \n",
            " Accuracy: 94.5%, Avg loss: 0.194165 \n",
            "\n",
            "epoch:  97\n",
            "Train loss: 0.059265\n",
            "Test loss: \n",
            " Accuracy: 94.5%, Avg loss: 0.193177 \n",
            "\n",
            "epoch:  98\n",
            "Train loss: 0.058878\n",
            "Test loss: \n",
            " Accuracy: 94.6%, Avg loss: 0.192201 \n",
            "\n",
            "epoch:  99\n",
            "Train loss: 0.058508\n",
            "Test loss: \n",
            " Accuracy: 94.6%, Avg loss: 0.191234 \n",
            "\n",
            "epoch:  100\n",
            "Train loss: 0.058146\n",
            "Test loss: \n",
            " Accuracy: 94.6%, Avg loss: 0.190270 \n",
            "\n",
            "epoch:  101\n",
            "Train loss: 0.057776\n",
            "Test loss: \n",
            " Accuracy: 94.7%, Avg loss: 0.189317 \n",
            "\n",
            "epoch:  102\n",
            "Train loss: 0.057397\n",
            "Test loss: \n",
            " Accuracy: 94.7%, Avg loss: 0.188371 \n",
            "\n",
            "epoch:  103\n",
            "Train loss: 0.057028\n",
            "Test loss: \n",
            " Accuracy: 94.7%, Avg loss: 0.187432 \n",
            "\n",
            "epoch:  104\n",
            "Train loss: 0.056667\n",
            "Test loss: \n",
            " Accuracy: 94.7%, Avg loss: 0.186502 \n",
            "\n",
            "epoch:  105\n",
            "Train loss: 0.056306\n",
            "Test loss: \n",
            " Accuracy: 94.7%, Avg loss: 0.185582 \n",
            "\n",
            "epoch:  106\n",
            "Train loss: 0.055936\n",
            "Test loss: \n",
            " Accuracy: 94.8%, Avg loss: 0.184671 \n",
            "\n",
            "epoch:  107\n",
            "Train loss: 0.055565\n",
            "Test loss: \n",
            " Accuracy: 94.8%, Avg loss: 0.183765 \n",
            "\n",
            "epoch:  108\n",
            "Train loss: 0.055194\n",
            "Test loss: \n",
            " Accuracy: 94.8%, Avg loss: 0.182868 \n",
            "\n",
            "epoch:  109\n",
            "Train loss: 0.054833\n",
            "Test loss: \n",
            " Accuracy: 94.8%, Avg loss: 0.181979 \n",
            "\n",
            "epoch:  110\n",
            "Train loss: 0.054489\n",
            "Test loss: \n",
            " Accuracy: 94.8%, Avg loss: 0.181094 \n",
            "\n",
            "epoch:  111\n",
            "Train loss: 0.054155\n",
            "Test loss: \n",
            " Accuracy: 94.8%, Avg loss: 0.180215 \n",
            "\n",
            "epoch:  112\n",
            "Train loss: 0.053829\n",
            "Test loss: \n",
            " Accuracy: 94.8%, Avg loss: 0.179343 \n",
            "\n",
            "epoch:  113\n",
            "Train loss: 0.053521\n",
            "Test loss: \n",
            " Accuracy: 94.8%, Avg loss: 0.178485 \n",
            "\n",
            "epoch:  114\n",
            "Train loss: 0.053208\n",
            "Test loss: \n",
            " Accuracy: 94.9%, Avg loss: 0.177634 \n",
            "\n",
            "epoch:  115\n",
            "Train loss: 0.052894\n",
            "Test loss: \n",
            " Accuracy: 94.9%, Avg loss: 0.176787 \n",
            "\n",
            "epoch:  116\n",
            "Train loss: 0.052618\n",
            "Test loss: \n",
            " Accuracy: 94.9%, Avg loss: 0.175946 \n",
            "\n",
            "epoch:  117\n",
            "Train loss: 0.052342\n",
            "Test loss: \n",
            " Accuracy: 94.9%, Avg loss: 0.175115 \n",
            "\n",
            "epoch:  118\n",
            "Train loss: 0.052055\n",
            "Test loss: \n",
            " Accuracy: 95.0%, Avg loss: 0.174295 \n",
            "\n",
            "epoch:  119\n",
            "Train loss: 0.051790\n",
            "Test loss: \n",
            " Accuracy: 95.0%, Avg loss: 0.173484 \n",
            "\n",
            "epoch:  120\n",
            "Train loss: 0.051520\n",
            "Test loss: \n",
            " Accuracy: 95.0%, Avg loss: 0.172677 \n",
            "\n",
            "epoch:  121\n",
            "Train loss: 0.051260\n",
            "Test loss: \n",
            " Accuracy: 95.0%, Avg loss: 0.171876 \n",
            "\n",
            "epoch:  122\n",
            "Train loss: 0.050998\n",
            "Test loss: \n",
            " Accuracy: 95.0%, Avg loss: 0.171079 \n",
            "\n",
            "epoch:  123\n",
            "Train loss: 0.050740\n",
            "Test loss: \n",
            " Accuracy: 95.0%, Avg loss: 0.170299 \n",
            "\n",
            "epoch:  124\n",
            "Train loss: 0.050482\n",
            "Test loss: \n",
            " Accuracy: 95.1%, Avg loss: 0.169521 \n",
            "\n",
            "epoch:  125\n",
            "Train loss: 0.050216\n",
            "Test loss: \n",
            " Accuracy: 95.1%, Avg loss: 0.168753 \n",
            "\n",
            "epoch:  126\n",
            "Train loss: 0.049968\n",
            "Test loss: \n",
            " Accuracy: 95.1%, Avg loss: 0.167986 \n",
            "\n",
            "epoch:  127\n",
            "Train loss: 0.049728\n",
            "Test loss: \n",
            " Accuracy: 95.1%, Avg loss: 0.167232 \n",
            "\n",
            "epoch:  128\n",
            "Train loss: 0.049485\n",
            "Test loss: \n",
            " Accuracy: 95.2%, Avg loss: 0.166482 \n",
            "\n",
            "epoch:  129\n",
            "Train loss: 0.049239\n",
            "Test loss: \n",
            " Accuracy: 95.2%, Avg loss: 0.165736 \n",
            "\n",
            "epoch:  130\n",
            "Train loss: 0.049007\n",
            "Test loss: \n",
            " Accuracy: 95.2%, Avg loss: 0.164993 \n",
            "\n",
            "epoch:  131\n",
            "Train loss: 0.048773\n",
            "Test loss: \n",
            " Accuracy: 95.2%, Avg loss: 0.164261 \n",
            "\n",
            "epoch:  132\n",
            "Train loss: 0.048513\n",
            "Test loss: \n",
            " Accuracy: 95.2%, Avg loss: 0.163534 \n",
            "\n",
            "epoch:  133\n",
            "Train loss: 0.048212\n",
            "Test loss: \n",
            " Accuracy: 95.2%, Avg loss: 0.162818 \n",
            "\n",
            "epoch:  134\n",
            "Train loss: 0.047904\n",
            "Test loss: \n",
            " Accuracy: 95.3%, Avg loss: 0.162106 \n",
            "\n",
            "epoch:  135\n",
            "Train loss: 0.047603\n",
            "Test loss: \n",
            " Accuracy: 95.3%, Avg loss: 0.161403 \n",
            "\n",
            "epoch:  136\n",
            "Train loss: 0.047317\n",
            "Test loss: \n",
            " Accuracy: 95.3%, Avg loss: 0.160710 \n",
            "\n",
            "epoch:  137\n",
            "Train loss: 0.047032\n",
            "Test loss: \n",
            " Accuracy: 95.3%, Avg loss: 0.160021 \n",
            "\n",
            "epoch:  138\n",
            "Train loss: 0.046732\n",
            "Test loss: \n",
            " Accuracy: 95.3%, Avg loss: 0.159337 \n",
            "\n",
            "epoch:  139\n",
            "Train loss: 0.046439\n",
            "Test loss: \n",
            " Accuracy: 95.4%, Avg loss: 0.158655 \n",
            "\n",
            "epoch:  140\n",
            "Train loss: 0.046143\n",
            "Test loss: \n",
            " Accuracy: 95.4%, Avg loss: 0.157979 \n",
            "\n",
            "epoch:  141\n",
            "Train loss: 0.045867\n",
            "Test loss: \n",
            " Accuracy: 95.4%, Avg loss: 0.157311 \n",
            "\n",
            "epoch:  142\n",
            "Train loss: 0.045583\n",
            "Test loss: \n",
            " Accuracy: 95.4%, Avg loss: 0.156647 \n",
            "\n",
            "epoch:  143\n",
            "Train loss: 0.045310\n",
            "Test loss: \n",
            " Accuracy: 95.4%, Avg loss: 0.155991 \n",
            "\n",
            "epoch:  144\n",
            "Train loss: 0.045036\n",
            "Test loss: \n",
            " Accuracy: 95.4%, Avg loss: 0.155344 \n",
            "\n",
            "epoch:  145\n",
            "Train loss: 0.044769\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.154705 \n",
            "\n",
            "epoch:  146\n",
            "Train loss: 0.044520\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.154070 \n",
            "\n",
            "epoch:  147\n",
            "Train loss: 0.044252\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.153442 \n",
            "\n",
            "epoch:  148\n",
            "Train loss: 0.043989\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.152817 \n",
            "\n",
            "epoch:  149\n",
            "Train loss: 0.043738\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.152200 \n",
            "\n",
            "epoch:  150\n",
            "Train loss: 0.043491\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.151586 \n",
            "\n",
            "epoch:  151\n",
            "Train loss: 0.043243\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.150977 \n",
            "\n",
            "epoch:  152\n",
            "Train loss: 0.043004\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.150371 \n",
            "\n",
            "epoch:  153\n",
            "Train loss: 0.042778\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.149775 \n",
            "\n",
            "epoch:  154\n",
            "Train loss: 0.042541\n",
            "Test loss: \n",
            " Accuracy: 95.5%, Avg loss: 0.149186 \n",
            "\n",
            "epoch:  155\n",
            "Train loss: 0.042314\n",
            "Test loss: \n",
            " Accuracy: 95.6%, Avg loss: 0.148597 \n",
            "\n",
            "epoch:  156\n",
            "Train loss: 0.042096\n",
            "Test loss: \n",
            " Accuracy: 95.6%, Avg loss: 0.148015 \n",
            "\n",
            "epoch:  157\n",
            "Train loss: 0.041879\n",
            "Test loss: \n",
            " Accuracy: 95.6%, Avg loss: 0.147439 \n",
            "\n",
            "epoch:  158\n",
            "Train loss: 0.041659\n",
            "Test loss: \n",
            " Accuracy: 95.6%, Avg loss: 0.146869 \n",
            "\n",
            "epoch:  159\n",
            "Train loss: 0.041453\n",
            "Test loss: \n",
            " Accuracy: 95.7%, Avg loss: 0.146301 \n",
            "\n",
            "epoch:  160\n",
            "Train loss: 0.041241\n",
            "Test loss: \n",
            " Accuracy: 95.7%, Avg loss: 0.145734 \n",
            "\n",
            "epoch:  161\n",
            "Train loss: 0.041013\n",
            "Test loss: \n",
            " Accuracy: 95.7%, Avg loss: 0.145172 \n",
            "\n",
            "epoch:  162\n",
            "Train loss: 0.040805\n",
            "Test loss: \n",
            " Accuracy: 95.7%, Avg loss: 0.144616 \n",
            "\n",
            "epoch:  163\n",
            "Train loss: 0.040604\n",
            "Test loss: \n",
            " Accuracy: 95.7%, Avg loss: 0.144064 \n",
            "\n",
            "epoch:  164\n",
            "Train loss: 0.040407\n",
            "Test loss: \n",
            " Accuracy: 95.7%, Avg loss: 0.143520 \n",
            "\n",
            "epoch:  165\n",
            "Train loss: 0.040219\n",
            "Test loss: \n",
            " Accuracy: 95.7%, Avg loss: 0.142975 \n",
            "\n",
            "epoch:  166\n",
            "Train loss: 0.040021\n",
            "Test loss: \n",
            " Accuracy: 95.8%, Avg loss: 0.142436 \n",
            "\n",
            "epoch:  167\n",
            "Train loss: 0.039839\n",
            "Test loss: \n",
            " Accuracy: 95.8%, Avg loss: 0.141905 \n",
            "\n",
            "epoch:  168\n",
            "Train loss: 0.039650\n",
            "Test loss: \n",
            " Accuracy: 95.8%, Avg loss: 0.141377 \n",
            "\n",
            "epoch:  169\n",
            "Train loss: 0.039459\n",
            "Test loss: \n",
            " Accuracy: 95.8%, Avg loss: 0.140856 \n",
            "\n",
            "epoch:  170\n",
            "Train loss: 0.039263\n",
            "Test loss: \n",
            " Accuracy: 95.8%, Avg loss: 0.140342 \n",
            "\n",
            "epoch:  171\n",
            "Train loss: 0.039069\n",
            "Test loss: \n",
            " Accuracy: 95.8%, Avg loss: 0.139830 \n",
            "\n",
            "epoch:  172\n",
            "Train loss: 0.038874\n",
            "Test loss: \n",
            " Accuracy: 95.8%, Avg loss: 0.139322 \n",
            "\n",
            "epoch:  173\n",
            "Train loss: 0.038688\n",
            "Test loss: \n",
            " Accuracy: 95.9%, Avg loss: 0.138816 \n",
            "\n",
            "epoch:  174\n",
            "Train loss: 0.038501\n",
            "Test loss: \n",
            " Accuracy: 95.9%, Avg loss: 0.138316 \n",
            "\n",
            "epoch:  175\n",
            "Train loss: 0.038294\n",
            "Test loss: \n",
            " Accuracy: 95.9%, Avg loss: 0.137820 \n",
            "\n",
            "epoch:  176\n",
            "Train loss: 0.038098\n",
            "Test loss: \n",
            " Accuracy: 95.9%, Avg loss: 0.137329 \n",
            "\n",
            "epoch:  177\n",
            "Train loss: 0.037894\n",
            "Test loss: \n",
            " Accuracy: 95.9%, Avg loss: 0.136837 \n",
            "\n",
            "epoch:  178\n",
            "Train loss: 0.037681\n",
            "Test loss: \n",
            " Accuracy: 96.0%, Avg loss: 0.136350 \n",
            "\n",
            "epoch:  179\n",
            "Train loss: 0.037488\n",
            "Test loss: \n",
            " Accuracy: 96.0%, Avg loss: 0.135866 \n",
            "\n",
            "epoch:  180\n",
            "Train loss: 0.037287\n",
            "Test loss: \n",
            " Accuracy: 96.0%, Avg loss: 0.135386 \n",
            "\n",
            "epoch:  181\n",
            "Train loss: 0.037080\n",
            "Test loss: \n",
            " Accuracy: 96.0%, Avg loss: 0.134913 \n",
            "\n",
            "epoch:  182\n",
            "Train loss: 0.036888\n",
            "Test loss: \n",
            " Accuracy: 96.0%, Avg loss: 0.134447 \n",
            "\n",
            "epoch:  183\n",
            "Train loss: 0.036679\n",
            "Test loss: \n",
            " Accuracy: 96.0%, Avg loss: 0.133983 \n",
            "\n",
            "epoch:  184\n",
            "Train loss: 0.036489\n",
            "Test loss: \n",
            " Accuracy: 96.0%, Avg loss: 0.133522 \n",
            "\n",
            "epoch:  185\n",
            "Train loss: 0.036308\n",
            "Test loss: \n",
            " Accuracy: 96.1%, Avg loss: 0.133066 \n",
            "\n",
            "epoch:  186\n",
            "Train loss: 0.036123\n",
            "Test loss: \n",
            " Accuracy: 96.1%, Avg loss: 0.132617 \n",
            "\n",
            "epoch:  187\n",
            "Train loss: 0.035929\n",
            "Test loss: \n",
            " Accuracy: 96.1%, Avg loss: 0.132167 \n",
            "\n",
            "epoch:  188\n",
            "Train loss: 0.035744\n",
            "Test loss: \n",
            " Accuracy: 96.1%, Avg loss: 0.131727 \n",
            "\n",
            "epoch:  189\n",
            "Train loss: 0.035566\n",
            "Test loss: \n",
            " Accuracy: 96.1%, Avg loss: 0.131286 \n",
            "\n",
            "epoch:  190\n",
            "Train loss: 0.035390\n",
            "Test loss: \n",
            " Accuracy: 96.2%, Avg loss: 0.130849 \n",
            "\n",
            "epoch:  191\n",
            "Train loss: 0.035206\n",
            "Test loss: \n",
            " Accuracy: 96.2%, Avg loss: 0.130415 \n",
            "\n",
            "epoch:  192\n",
            "Train loss: 0.035022\n",
            "Test loss: \n",
            " Accuracy: 96.2%, Avg loss: 0.129989 \n",
            "\n",
            "epoch:  193\n",
            "Train loss: 0.034846\n",
            "Test loss: \n",
            " Accuracy: 96.2%, Avg loss: 0.129565 \n",
            "\n",
            "epoch:  194\n",
            "Train loss: 0.034674\n",
            "Test loss: \n",
            " Accuracy: 96.2%, Avg loss: 0.129142 \n",
            "\n",
            "epoch:  195\n",
            "Train loss: 0.034507\n",
            "Test loss: \n",
            " Accuracy: 96.2%, Avg loss: 0.128726 \n",
            "\n",
            "epoch:  196\n",
            "Train loss: 0.034325\n",
            "Test loss: \n",
            " Accuracy: 96.2%, Avg loss: 0.128311 \n",
            "\n",
            "epoch:  197\n",
            "Train loss: 0.034143\n",
            "Test loss: \n",
            " Accuracy: 96.2%, Avg loss: 0.127896 \n",
            "\n",
            "epoch:  198\n",
            "Train loss: 0.033962\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.127491 \n",
            "\n",
            "epoch:  199\n",
            "Train loss: 0.033785\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.127085 \n",
            "\n",
            "epoch:  200\n",
            "Train loss: 0.033603\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.126682 \n",
            "\n",
            "epoch:  201\n",
            "Train loss: 0.033432\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.126281 \n",
            "\n",
            "epoch:  202\n",
            "Train loss: 0.033259\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.125879 \n",
            "\n",
            "epoch:  203\n",
            "Train loss: 0.033082\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.125484 \n",
            "\n",
            "epoch:  204\n",
            "Train loss: 0.032902\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.125087 \n",
            "\n",
            "epoch:  205\n",
            "Train loss: 0.032737\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.124695 \n",
            "\n",
            "epoch:  206\n",
            "Train loss: 0.032581\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.124306 \n",
            "\n",
            "epoch:  207\n",
            "Train loss: 0.032420\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.123920 \n",
            "\n",
            "epoch:  208\n",
            "Train loss: 0.032250\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.123536 \n",
            "\n",
            "epoch:  209\n",
            "Train loss: 0.032080\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.123155 \n",
            "\n",
            "epoch:  210\n",
            "Train loss: 0.031916\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.122780 \n",
            "\n",
            "epoch:  211\n",
            "Train loss: 0.031762\n",
            "Test loss: \n",
            " Accuracy: 96.3%, Avg loss: 0.122404 \n",
            "\n",
            "epoch:  212\n",
            "Train loss: 0.031619\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.122033 \n",
            "\n",
            "epoch:  213\n",
            "Train loss: 0.031476\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.121667 \n",
            "\n",
            "epoch:  214\n",
            "Train loss: 0.031344\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.121302 \n",
            "\n",
            "epoch:  215\n",
            "Train loss: 0.031212\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.120939 \n",
            "\n",
            "epoch:  216\n",
            "Train loss: 0.031059\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.120580 \n",
            "\n",
            "epoch:  217\n",
            "Train loss: 0.030926\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.120223 \n",
            "\n",
            "epoch:  218\n",
            "Train loss: 0.030792\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.119870 \n",
            "\n",
            "epoch:  219\n",
            "Train loss: 0.030642\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.119518 \n",
            "\n",
            "epoch:  220\n",
            "Train loss: 0.030489\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.119171 \n",
            "\n",
            "epoch:  221\n",
            "Train loss: 0.030344\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.118826 \n",
            "\n",
            "epoch:  222\n",
            "Train loss: 0.030195\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.118482 \n",
            "\n",
            "epoch:  223\n",
            "Train loss: 0.030051\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.118146 \n",
            "\n",
            "epoch:  224\n",
            "Train loss: 0.029907\n",
            "Test loss: \n",
            " Accuracy: 96.4%, Avg loss: 0.117811 \n",
            "\n",
            "epoch:  225\n",
            "Train loss: 0.029761\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.117477 \n",
            "\n",
            "epoch:  226\n",
            "Train loss: 0.029610\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.117146 \n",
            "\n",
            "epoch:  227\n",
            "Train loss: 0.029464\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.116821 \n",
            "\n",
            "epoch:  228\n",
            "Train loss: 0.029310\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.116494 \n",
            "\n",
            "epoch:  229\n",
            "Train loss: 0.029161\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.116172 \n",
            "\n",
            "epoch:  230\n",
            "Train loss: 0.029012\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.115854 \n",
            "\n",
            "epoch:  231\n",
            "Train loss: 0.028860\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.115535 \n",
            "\n",
            "epoch:  232\n",
            "Train loss: 0.028713\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.115219 \n",
            "\n",
            "epoch:  233\n",
            "Train loss: 0.028574\n",
            "Test loss: \n",
            " Accuracy: 96.5%, Avg loss: 0.114906 \n",
            "\n",
            "epoch:  234\n",
            "Train loss: 0.028437\n",
            "Test loss: \n",
            " Accuracy: 96.6%, Avg loss: 0.114596 \n",
            "\n",
            "epoch:  235\n",
            "Train loss: 0.028301\n",
            "Test loss: \n",
            " Accuracy: 96.6%, Avg loss: 0.114289 \n",
            "\n",
            "epoch:  236\n",
            "Train loss: 0.028174\n",
            "Test loss: \n",
            " Accuracy: 96.6%, Avg loss: 0.113985 \n",
            "\n",
            "epoch:  237\n",
            "Train loss: 0.028043\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.113686 \n",
            "\n",
            "epoch:  238\n",
            "Train loss: 0.027907\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.113388 \n",
            "\n",
            "epoch:  239\n",
            "Train loss: 0.027786\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.113091 \n",
            "\n",
            "epoch:  240\n",
            "Train loss: 0.027653\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.112796 \n",
            "\n",
            "epoch:  241\n",
            "Train loss: 0.027521\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.112501 \n",
            "\n",
            "epoch:  242\n",
            "Train loss: 0.027399\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.112205 \n",
            "\n",
            "epoch:  243\n",
            "Train loss: 0.027297\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.111910 \n",
            "\n",
            "epoch:  244\n",
            "Train loss: 0.027183\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.111616 \n",
            "\n",
            "epoch:  245\n",
            "Train loss: 0.027086\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.111325 \n",
            "\n",
            "epoch:  246\n",
            "Train loss: 0.026991\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.111037 \n",
            "\n",
            "epoch:  247\n",
            "Train loss: 0.026888\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.110752 \n",
            "\n",
            "epoch:  248\n",
            "Train loss: 0.026783\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.110467 \n",
            "\n",
            "epoch:  249\n",
            "Train loss: 0.026684\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.110187 \n",
            "\n",
            "epoch:  250\n",
            "Train loss: 0.026578\n",
            "Test loss: \n",
            " Accuracy: 96.7%, Avg loss: 0.109911 \n",
            "\n",
            "epoch:  251\n",
            "Train loss: 0.026498\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.109637 \n",
            "\n",
            "epoch:  252\n",
            "Train loss: 0.026410\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.109367 \n",
            "\n",
            "epoch:  253\n",
            "Train loss: 0.026323\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.109097 \n",
            "\n",
            "epoch:  254\n",
            "Train loss: 0.026233\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.108830 \n",
            "\n",
            "epoch:  255\n",
            "Train loss: 0.026156\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.108562 \n",
            "\n",
            "epoch:  256\n",
            "Train loss: 0.026069\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.108293 \n",
            "\n",
            "epoch:  257\n",
            "Train loss: 0.025995\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.108032 \n",
            "\n",
            "epoch:  258\n",
            "Train loss: 0.025928\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.107769 \n",
            "\n",
            "epoch:  259\n",
            "Train loss: 0.025845\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.107506 \n",
            "\n",
            "epoch:  260\n",
            "Train loss: 0.025762\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.107247 \n",
            "\n",
            "epoch:  261\n",
            "Train loss: 0.025666\n",
            "Test loss: \n",
            " Accuracy: 96.8%, Avg loss: 0.106988 \n",
            "\n",
            "epoch:  262\n",
            "Train loss: 0.025576\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.106734 \n",
            "\n",
            "epoch:  263\n",
            "Train loss: 0.025493\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.106478 \n",
            "\n",
            "epoch:  264\n",
            "Train loss: 0.025414\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.106228 \n",
            "\n",
            "epoch:  265\n",
            "Train loss: 0.025329\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.105974 \n",
            "\n",
            "epoch:  266\n",
            "Train loss: 0.025255\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.105726 \n",
            "\n",
            "epoch:  267\n",
            "Train loss: 0.025174\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.105479 \n",
            "\n",
            "epoch:  268\n",
            "Train loss: 0.025103\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.105235 \n",
            "\n",
            "epoch:  269\n",
            "Train loss: 0.025043\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.104992 \n",
            "\n",
            "epoch:  270\n",
            "Train loss: 0.024981\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.104753 \n",
            "\n",
            "epoch:  271\n",
            "Train loss: 0.024911\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.104509 \n",
            "\n",
            "epoch:  272\n",
            "Train loss: 0.024831\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.104270 \n",
            "\n",
            "epoch:  273\n",
            "Train loss: 0.024777\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.104029 \n",
            "\n",
            "epoch:  274\n",
            "Train loss: 0.024700\n",
            "Test loss: \n",
            " Accuracy: 96.9%, Avg loss: 0.103789 \n",
            "\n",
            "epoch:  275\n",
            "Train loss: 0.024633\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.103553 \n",
            "\n",
            "epoch:  276\n",
            "Train loss: 0.024565\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.103316 \n",
            "\n",
            "epoch:  277\n",
            "Train loss: 0.024485\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.103081 \n",
            "\n",
            "epoch:  278\n",
            "Train loss: 0.024405\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.102850 \n",
            "\n",
            "epoch:  279\n",
            "Train loss: 0.024323\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.102616 \n",
            "\n",
            "epoch:  280\n",
            "Train loss: 0.024254\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.102388 \n",
            "\n",
            "epoch:  281\n",
            "Train loss: 0.024189\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.102159 \n",
            "\n",
            "epoch:  282\n",
            "Train loss: 0.024113\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.101934 \n",
            "\n",
            "epoch:  283\n",
            "Train loss: 0.024044\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.101709 \n",
            "\n",
            "epoch:  284\n",
            "Train loss: 0.023973\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.101487 \n",
            "\n",
            "epoch:  285\n",
            "Train loss: 0.023887\n",
            "Test loss: \n",
            " Accuracy: 97.0%, Avg loss: 0.101266 \n",
            "\n",
            "epoch:  286\n",
            "Train loss: 0.023819\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.101049 \n",
            "\n",
            "epoch:  287\n",
            "Train loss: 0.023740\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.100834 \n",
            "\n",
            "epoch:  288\n",
            "Train loss: 0.023675\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.100622 \n",
            "\n",
            "epoch:  289\n",
            "Train loss: 0.023617\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.100413 \n",
            "\n",
            "epoch:  290\n",
            "Train loss: 0.023555\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.100197 \n",
            "\n",
            "epoch:  291\n",
            "Train loss: 0.023495\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.099991 \n",
            "\n",
            "epoch:  292\n",
            "Train loss: 0.023435\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.099784 \n",
            "\n",
            "epoch:  293\n",
            "Train loss: 0.023378\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.099574 \n",
            "\n",
            "epoch:  294\n",
            "Train loss: 0.023317\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.099369 \n",
            "\n",
            "epoch:  295\n",
            "Train loss: 0.023263\n",
            "Test loss: \n",
            " Accuracy: 97.1%, Avg loss: 0.099168 \n",
            "\n",
            "epoch:  296\n",
            "Train loss: 0.023196\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.098961 \n",
            "\n",
            "epoch:  297\n",
            "Train loss: 0.023141\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.098759 \n",
            "\n",
            "epoch:  298\n",
            "Train loss: 0.023087\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.098559 \n",
            "\n",
            "epoch:  299\n",
            "Train loss: 0.023032\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.098360 \n",
            "\n",
            "epoch:  300\n",
            "Train loss: 0.022974\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.098162 \n",
            "\n",
            "epoch:  301\n",
            "Train loss: 0.022918\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.097964 \n",
            "\n",
            "epoch:  302\n",
            "Train loss: 0.022851\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.097767 \n",
            "\n",
            "epoch:  303\n",
            "Train loss: 0.022791\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.097572 \n",
            "\n",
            "epoch:  304\n",
            "Train loss: 0.022717\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.097382 \n",
            "\n",
            "epoch:  305\n",
            "Train loss: 0.022653\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.097191 \n",
            "\n",
            "epoch:  306\n",
            "Train loss: 0.022592\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.097003 \n",
            "\n",
            "epoch:  307\n",
            "Train loss: 0.022536\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.096815 \n",
            "\n",
            "epoch:  308\n",
            "Train loss: 0.022479\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.096629 \n",
            "\n",
            "epoch:  309\n",
            "Train loss: 0.022434\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.096446 \n",
            "\n",
            "epoch:  310\n",
            "Train loss: 0.022388\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.096264 \n",
            "\n",
            "epoch:  311\n",
            "Train loss: 0.022337\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.096080 \n",
            "\n",
            "epoch:  312\n",
            "Train loss: 0.022293\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.095896 \n",
            "\n",
            "epoch:  313\n",
            "Train loss: 0.022237\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.095714 \n",
            "\n",
            "epoch:  314\n",
            "Train loss: 0.022179\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.095533 \n",
            "\n",
            "epoch:  315\n",
            "Train loss: 0.022120\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.095355 \n",
            "\n",
            "epoch:  316\n",
            "Train loss: 0.022067\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.095175 \n",
            "\n",
            "epoch:  317\n",
            "Train loss: 0.022011\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.094995 \n",
            "\n",
            "epoch:  318\n",
            "Train loss: 0.021949\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.094814 \n",
            "\n",
            "epoch:  319\n",
            "Train loss: 0.021891\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.094636 \n",
            "\n",
            "epoch:  320\n",
            "Train loss: 0.021836\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.094458 \n",
            "\n",
            "epoch:  321\n",
            "Train loss: 0.021781\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.094286 \n",
            "\n",
            "epoch:  322\n",
            "Train loss: 0.021716\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.094110 \n",
            "\n",
            "epoch:  323\n",
            "Train loss: 0.021664\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.093941 \n",
            "\n",
            "epoch:  324\n",
            "Train loss: 0.021601\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.093765 \n",
            "\n",
            "epoch:  325\n",
            "Train loss: 0.021553\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.093597 \n",
            "\n",
            "epoch:  326\n",
            "Train loss: 0.021490\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.093429 \n",
            "\n",
            "epoch:  327\n",
            "Train loss: 0.021425\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.093265 \n",
            "\n",
            "epoch:  328\n",
            "Train loss: 0.021356\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.093101 \n",
            "\n",
            "epoch:  329\n",
            "Train loss: 0.021299\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.092940 \n",
            "\n",
            "epoch:  330\n",
            "Train loss: 0.021225\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.092773 \n",
            "\n",
            "epoch:  331\n",
            "Train loss: 0.021153\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.092609 \n",
            "\n",
            "epoch:  332\n",
            "Train loss: 0.021084\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.092450 \n",
            "\n",
            "epoch:  333\n",
            "Train loss: 0.021018\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.092290 \n",
            "\n",
            "epoch:  334\n",
            "Train loss: 0.020943\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.092130 \n",
            "\n",
            "epoch:  335\n",
            "Train loss: 0.020880\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.091971 \n",
            "\n",
            "epoch:  336\n",
            "Train loss: 0.020830\n",
            "Test loss: \n",
            " Accuracy: 97.2%, Avg loss: 0.091821 \n",
            "\n",
            "epoch:  337\n",
            "Train loss: 0.020751\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.091663 \n",
            "\n",
            "epoch:  338\n",
            "Train loss: 0.020693\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.091513 \n",
            "\n",
            "epoch:  339\n",
            "Train loss: 0.020625\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.091354 \n",
            "\n",
            "epoch:  340\n",
            "Train loss: 0.020558\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.091206 \n",
            "\n",
            "epoch:  341\n",
            "Train loss: 0.020490\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.091048 \n",
            "\n",
            "epoch:  342\n",
            "Train loss: 0.020407\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.090898 \n",
            "\n",
            "epoch:  343\n",
            "Train loss: 0.020334\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.090750 \n",
            "\n",
            "epoch:  344\n",
            "Train loss: 0.020245\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.090598 \n",
            "\n",
            "epoch:  345\n",
            "Train loss: 0.020180\n",
            "Test loss: \n",
            " Accuracy: 97.3%, Avg loss: 0.090455 \n",
            "\n",
            "epoch:  346\n",
            "Train loss: 0.020118\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.090308 \n",
            "\n",
            "epoch:  347\n",
            "Train loss: 0.020055\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.090159 \n",
            "\n",
            "epoch:  348\n",
            "Train loss: 0.019993\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.090016 \n",
            "\n",
            "epoch:  349\n",
            "Train loss: 0.019931\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.089872 \n",
            "\n",
            "epoch:  350\n",
            "Train loss: 0.019871\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.089727 \n",
            "\n",
            "epoch:  351\n",
            "Train loss: 0.019798\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.089584 \n",
            "\n",
            "epoch:  352\n",
            "Train loss: 0.019732\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.089439 \n",
            "\n",
            "epoch:  353\n",
            "Train loss: 0.019667\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.089296 \n",
            "\n",
            "epoch:  354\n",
            "Train loss: 0.019608\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.089152 \n",
            "\n",
            "epoch:  355\n",
            "Train loss: 0.019546\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.089013 \n",
            "\n",
            "epoch:  356\n",
            "Train loss: 0.019483\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.088876 \n",
            "\n",
            "epoch:  357\n",
            "Train loss: 0.019411\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.088741 \n",
            "\n",
            "epoch:  358\n",
            "Train loss: 0.019368\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.088606 \n",
            "\n",
            "epoch:  359\n",
            "Train loss: 0.019306\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.088471 \n",
            "\n",
            "epoch:  360\n",
            "Train loss: 0.019257\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.088337 \n",
            "\n",
            "epoch:  361\n",
            "Train loss: 0.019206\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.088199 \n",
            "\n",
            "epoch:  362\n",
            "Train loss: 0.019147\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.088064 \n",
            "\n",
            "epoch:  363\n",
            "Train loss: 0.019098\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.087933 \n",
            "\n",
            "epoch:  364\n",
            "Train loss: 0.019027\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.087798 \n",
            "\n",
            "epoch:  365\n",
            "Train loss: 0.018979\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.087668 \n",
            "\n",
            "epoch:  366\n",
            "Train loss: 0.018936\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.087537 \n",
            "\n",
            "epoch:  367\n",
            "Train loss: 0.018873\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.087407 \n",
            "\n",
            "epoch:  368\n",
            "Train loss: 0.018821\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.087275 \n",
            "\n",
            "epoch:  369\n",
            "Train loss: 0.018768\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.087150 \n",
            "\n",
            "epoch:  370\n",
            "Train loss: 0.018700\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.087020 \n",
            "\n",
            "epoch:  371\n",
            "Train loss: 0.018644\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.086891 \n",
            "\n",
            "epoch:  372\n",
            "Train loss: 0.018593\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.086767 \n",
            "\n",
            "epoch:  373\n",
            "Train loss: 0.018521\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.086638 \n",
            "\n",
            "epoch:  374\n",
            "Train loss: 0.018456\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.086509 \n",
            "\n",
            "epoch:  375\n",
            "Train loss: 0.018409\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.086389 \n",
            "\n",
            "epoch:  376\n",
            "Train loss: 0.018348\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.086260 \n",
            "\n",
            "epoch:  377\n",
            "Train loss: 0.018304\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.086138 \n",
            "\n",
            "epoch:  378\n",
            "Train loss: 0.018245\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.086018 \n",
            "\n",
            "epoch:  379\n",
            "Train loss: 0.018192\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.085891 \n",
            "\n",
            "epoch:  380\n",
            "Train loss: 0.018130\n",
            "Test loss: \n",
            " Accuracy: 97.4%, Avg loss: 0.085770 \n",
            "\n",
            "epoch:  381\n",
            "Train loss: 0.018062\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.085647 \n",
            "\n",
            "epoch:  382\n",
            "Train loss: 0.018019\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.085527 \n",
            "\n",
            "epoch:  383\n",
            "Train loss: 0.017953\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.085409 \n",
            "\n",
            "epoch:  384\n",
            "Train loss: 0.017909\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.085288 \n",
            "\n",
            "epoch:  385\n",
            "Train loss: 0.017849\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.085177 \n",
            "\n",
            "epoch:  386\n",
            "Train loss: 0.017777\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.085057 \n",
            "\n",
            "epoch:  387\n",
            "Train loss: 0.017722\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084944 \n",
            "\n",
            "epoch:  388\n",
            "Train loss: 0.017666\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084829 \n",
            "\n",
            "epoch:  389\n",
            "Train loss: 0.017611\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084721 \n",
            "\n",
            "epoch:  390\n",
            "Train loss: 0.017544\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084604 \n",
            "\n",
            "epoch:  391\n",
            "Train loss: 0.017477\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084489 \n",
            "\n",
            "epoch:  392\n",
            "Train loss: 0.017430\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084376 \n",
            "\n",
            "epoch:  393\n",
            "Train loss: 0.017369\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084266 \n",
            "\n",
            "epoch:  394\n",
            "Train loss: 0.017296\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084152 \n",
            "\n",
            "epoch:  395\n",
            "Train loss: 0.017262\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.084046 \n",
            "\n",
            "epoch:  396\n",
            "Train loss: 0.017203\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083936 \n",
            "\n",
            "epoch:  397\n",
            "Train loss: 0.017146\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083825 \n",
            "\n",
            "epoch:  398\n",
            "Train loss: 0.017090\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083717 \n",
            "\n",
            "epoch:  399\n",
            "Train loss: 0.017024\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083610 \n",
            "\n",
            "epoch:  400\n",
            "Train loss: 0.016961\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083504 \n",
            "\n",
            "epoch:  401\n",
            "Train loss: 0.016915\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083394 \n",
            "\n",
            "epoch:  402\n",
            "Train loss: 0.016861\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083291 \n",
            "\n",
            "epoch:  403\n",
            "Train loss: 0.016803\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083185 \n",
            "\n",
            "epoch:  404\n",
            "Train loss: 0.016767\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.083085 \n",
            "\n",
            "epoch:  405\n",
            "Train loss: 0.016694\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082970 \n",
            "\n",
            "epoch:  406\n",
            "Train loss: 0.016655\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082876 \n",
            "\n",
            "epoch:  407\n",
            "Train loss: 0.016599\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082770 \n",
            "\n",
            "epoch:  408\n",
            "Train loss: 0.016554\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082672 \n",
            "\n",
            "epoch:  409\n",
            "Train loss: 0.016515\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082573 \n",
            "\n",
            "epoch:  410\n",
            "Train loss: 0.016468\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082473 \n",
            "\n",
            "epoch:  411\n",
            "Train loss: 0.016413\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082380 \n",
            "\n",
            "epoch:  412\n",
            "Train loss: 0.016351\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082276 \n",
            "\n",
            "epoch:  413\n",
            "Train loss: 0.016326\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082179 \n",
            "\n",
            "epoch:  414\n",
            "Train loss: 0.016278\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.082082 \n",
            "\n",
            "epoch:  415\n",
            "Train loss: 0.016231\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081987 \n",
            "\n",
            "epoch:  416\n",
            "Train loss: 0.016180\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081888 \n",
            "\n",
            "epoch:  417\n",
            "Train loss: 0.016153\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081797 \n",
            "\n",
            "epoch:  418\n",
            "Train loss: 0.016102\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081699 \n",
            "\n",
            "epoch:  419\n",
            "Train loss: 0.016061\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081601 \n",
            "\n",
            "epoch:  420\n",
            "Train loss: 0.016028\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081510 \n",
            "\n",
            "epoch:  421\n",
            "Train loss: 0.015989\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081414 \n",
            "\n",
            "epoch:  422\n",
            "Train loss: 0.015935\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081322 \n",
            "\n",
            "epoch:  423\n",
            "Train loss: 0.015899\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081224 \n",
            "\n",
            "epoch:  424\n",
            "Train loss: 0.015862\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081137 \n",
            "\n",
            "epoch:  425\n",
            "Train loss: 0.015794\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.081043 \n",
            "\n",
            "epoch:  426\n",
            "Train loss: 0.015755\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.080955 \n",
            "\n",
            "epoch:  427\n",
            "Train loss: 0.015705\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.080862 \n",
            "\n",
            "epoch:  428\n",
            "Train loss: 0.015660\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.080780 \n",
            "\n",
            "epoch:  429\n",
            "Train loss: 0.015622\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.080696 \n",
            "\n",
            "epoch:  430\n",
            "Train loss: 0.015566\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.080605 \n",
            "\n",
            "epoch:  431\n",
            "Train loss: 0.015524\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.080521 \n",
            "\n",
            "epoch:  432\n",
            "Train loss: 0.015473\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.080434 \n",
            "\n",
            "epoch:  433\n",
            "Train loss: 0.015444\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.080349 \n",
            "\n",
            "epoch:  434\n",
            "Train loss: 0.015383\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.080263 \n",
            "\n",
            "epoch:  435\n",
            "Train loss: 0.015347\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.080179 \n",
            "\n",
            "epoch:  436\n",
            "Train loss: 0.015295\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.080086 \n",
            "\n",
            "epoch:  437\n",
            "Train loss: 0.015245\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.080000 \n",
            "\n",
            "epoch:  438\n",
            "Train loss: 0.015202\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.079919 \n",
            "\n",
            "epoch:  439\n",
            "Train loss: 0.015147\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.079834 \n",
            "\n",
            "epoch:  440\n",
            "Train loss: 0.015111\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.079747 \n",
            "\n",
            "epoch:  441\n",
            "Train loss: 0.015067\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.079661 \n",
            "\n",
            "epoch:  442\n",
            "Train loss: 0.015016\n",
            "Test loss: \n",
            " Accuracy: 97.5%, Avg loss: 0.079578 \n",
            "\n",
            "epoch:  443\n",
            "Train loss: 0.014983\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.079493 \n",
            "\n",
            "epoch:  444\n",
            "Train loss: 0.014938\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.079409 \n",
            "\n",
            "epoch:  445\n",
            "Train loss: 0.014891\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.079327 \n",
            "\n",
            "epoch:  446\n",
            "Train loss: 0.014854\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.079248 \n",
            "\n",
            "epoch:  447\n",
            "Train loss: 0.014808\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.079162 \n",
            "\n",
            "epoch:  448\n",
            "Train loss: 0.014755\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.079087 \n",
            "\n",
            "epoch:  449\n",
            "Train loss: 0.014716\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.079000 \n",
            "\n",
            "epoch:  450\n",
            "Train loss: 0.014663\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078918 \n",
            "\n",
            "epoch:  451\n",
            "Train loss: 0.014613\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078836 \n",
            "\n",
            "epoch:  452\n",
            "Train loss: 0.014562\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078756 \n",
            "\n",
            "epoch:  453\n",
            "Train loss: 0.014513\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078680 \n",
            "\n",
            "epoch:  454\n",
            "Train loss: 0.014466\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078601 \n",
            "\n",
            "epoch:  455\n",
            "Train loss: 0.014421\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078519 \n",
            "\n",
            "epoch:  456\n",
            "Train loss: 0.014376\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078445 \n",
            "\n",
            "epoch:  457\n",
            "Train loss: 0.014341\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078370 \n",
            "\n",
            "epoch:  458\n",
            "Train loss: 0.014283\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078289 \n",
            "\n",
            "epoch:  459\n",
            "Train loss: 0.014235\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078215 \n",
            "\n",
            "epoch:  460\n",
            "Train loss: 0.014192\n",
            "Test loss: \n",
            " Accuracy: 97.6%, Avg loss: 0.078139 \n",
            "\n",
            "epoch:  461\n",
            "Train loss: 0.014140\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.078067 \n",
            "\n",
            "epoch:  462\n",
            "Train loss: 0.014105\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077990 \n",
            "\n",
            "epoch:  463\n",
            "Train loss: 0.014064\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077917 \n",
            "\n",
            "epoch:  464\n",
            "Train loss: 0.014016\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077842 \n",
            "\n",
            "epoch:  465\n",
            "Train loss: 0.013963\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077771 \n",
            "\n",
            "epoch:  466\n",
            "Train loss: 0.013925\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077696 \n",
            "\n",
            "epoch:  467\n",
            "Train loss: 0.013875\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077626 \n",
            "\n",
            "epoch:  468\n",
            "Train loss: 0.013844\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077556 \n",
            "\n",
            "epoch:  469\n",
            "Train loss: 0.013787\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077484 \n",
            "\n",
            "epoch:  470\n",
            "Train loss: 0.013753\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077413 \n",
            "\n",
            "epoch:  471\n",
            "Train loss: 0.013706\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077341 \n",
            "\n",
            "epoch:  472\n",
            "Train loss: 0.013665\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077268 \n",
            "\n",
            "epoch:  473\n",
            "Train loss: 0.013629\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077194 \n",
            "\n",
            "epoch:  474\n",
            "Train loss: 0.013593\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077126 \n",
            "\n",
            "epoch:  475\n",
            "Train loss: 0.013552\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.077059 \n",
            "\n",
            "epoch:  476\n",
            "Train loss: 0.013518\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076985 \n",
            "\n",
            "epoch:  477\n",
            "Train loss: 0.013475\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076913 \n",
            "\n",
            "epoch:  478\n",
            "Train loss: 0.013435\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076848 \n",
            "\n",
            "epoch:  479\n",
            "Train loss: 0.013404\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076778 \n",
            "\n",
            "epoch:  480\n",
            "Train loss: 0.013373\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076712 \n",
            "\n",
            "epoch:  481\n",
            "Train loss: 0.013325\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076643 \n",
            "\n",
            "epoch:  482\n",
            "Train loss: 0.013311\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076576 \n",
            "\n",
            "epoch:  483\n",
            "Train loss: 0.013263\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076506 \n",
            "\n",
            "epoch:  484\n",
            "Train loss: 0.013234\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076441 \n",
            "\n",
            "epoch:  485\n",
            "Train loss: 0.013198\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076375 \n",
            "\n",
            "epoch:  486\n",
            "Train loss: 0.013153\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076304 \n",
            "\n",
            "epoch:  487\n",
            "Train loss: 0.013119\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076239 \n",
            "\n",
            "epoch:  488\n",
            "Train loss: 0.013097\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076177 \n",
            "\n",
            "epoch:  489\n",
            "Train loss: 0.013054\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076111 \n",
            "\n",
            "epoch:  490\n",
            "Train loss: 0.013030\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.076049 \n",
            "\n",
            "epoch:  491\n",
            "Train loss: 0.012985\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.075981 \n",
            "\n",
            "epoch:  492\n",
            "Train loss: 0.012954\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.075916 \n",
            "\n",
            "epoch:  493\n",
            "Train loss: 0.012924\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.075858 \n",
            "\n",
            "epoch:  494\n",
            "Train loss: 0.012888\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.075791 \n",
            "\n",
            "epoch:  495\n",
            "Train loss: 0.012858\n",
            "Test loss: \n",
            " Accuracy: 97.7%, Avg loss: 0.075729 \n",
            "\n",
            "epoch:  496\n",
            "Train loss: 0.012818\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075666 \n",
            "\n",
            "epoch:  497\n",
            "Train loss: 0.012781\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075602 \n",
            "\n",
            "epoch:  498\n",
            "Train loss: 0.012741\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075545 \n",
            "\n",
            "epoch:  499\n",
            "Train loss: 0.012714\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075485 \n",
            "\n",
            "epoch:  500\n",
            "Train loss: 0.012675\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075425 \n",
            "\n",
            "epoch:  501\n",
            "Train loss: 0.012643\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075367 \n",
            "\n",
            "epoch:  502\n",
            "Train loss: 0.012611\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075305 \n",
            "\n",
            "epoch:  503\n",
            "Train loss: 0.012583\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075249 \n",
            "\n",
            "epoch:  504\n",
            "Train loss: 0.012559\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075191 \n",
            "\n",
            "epoch:  505\n",
            "Train loss: 0.012520\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075130 \n",
            "\n",
            "epoch:  506\n",
            "Train loss: 0.012489\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075072 \n",
            "\n",
            "epoch:  507\n",
            "Train loss: 0.012455\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.075011 \n",
            "\n",
            "epoch:  508\n",
            "Train loss: 0.012414\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074958 \n",
            "\n",
            "epoch:  509\n",
            "Train loss: 0.012394\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074898 \n",
            "\n",
            "epoch:  510\n",
            "Train loss: 0.012348\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074841 \n",
            "\n",
            "epoch:  511\n",
            "Train loss: 0.012323\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074778 \n",
            "\n",
            "epoch:  512\n",
            "Train loss: 0.012287\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074724 \n",
            "\n",
            "epoch:  513\n",
            "Train loss: 0.012255\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074664 \n",
            "\n",
            "epoch:  514\n",
            "Train loss: 0.012230\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074608 \n",
            "\n",
            "epoch:  515\n",
            "Train loss: 0.012182\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074557 \n",
            "\n",
            "epoch:  516\n",
            "Train loss: 0.012153\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074501 \n",
            "\n",
            "epoch:  517\n",
            "Train loss: 0.012118\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074443 \n",
            "\n",
            "epoch:  518\n",
            "Train loss: 0.012094\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074389 \n",
            "\n",
            "epoch:  519\n",
            "Train loss: 0.012057\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074331 \n",
            "\n",
            "epoch:  520\n",
            "Train loss: 0.012033\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074281 \n",
            "\n",
            "epoch:  521\n",
            "Train loss: 0.011994\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074222 \n",
            "\n",
            "epoch:  522\n",
            "Train loss: 0.011970\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074170 \n",
            "\n",
            "epoch:  523\n",
            "Train loss: 0.011936\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074113 \n",
            "\n",
            "epoch:  524\n",
            "Train loss: 0.011909\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074056 \n",
            "\n",
            "epoch:  525\n",
            "Train loss: 0.011890\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.074005 \n",
            "\n",
            "epoch:  526\n",
            "Train loss: 0.011846\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073954 \n",
            "\n",
            "epoch:  527\n",
            "Train loss: 0.011819\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073902 \n",
            "\n",
            "epoch:  528\n",
            "Train loss: 0.011788\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073852 \n",
            "\n",
            "epoch:  529\n",
            "Train loss: 0.011768\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073803 \n",
            "\n",
            "epoch:  530\n",
            "Train loss: 0.011726\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073752 \n",
            "\n",
            "epoch:  531\n",
            "Train loss: 0.011709\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073704 \n",
            "\n",
            "epoch:  532\n",
            "Train loss: 0.011687\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073654 \n",
            "\n",
            "epoch:  533\n",
            "Train loss: 0.011649\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073607 \n",
            "\n",
            "epoch:  534\n",
            "Train loss: 0.011635\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073557 \n",
            "\n",
            "epoch:  535\n",
            "Train loss: 0.011601\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073506 \n",
            "\n",
            "epoch:  536\n",
            "Train loss: 0.011576\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073462 \n",
            "\n",
            "epoch:  537\n",
            "Train loss: 0.011548\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073412 \n",
            "\n",
            "epoch:  538\n",
            "Train loss: 0.011514\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073365 \n",
            "\n",
            "epoch:  539\n",
            "Train loss: 0.011501\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073320 \n",
            "\n",
            "epoch:  540\n",
            "Train loss: 0.011471\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073267 \n",
            "\n",
            "epoch:  541\n",
            "Train loss: 0.011439\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073222 \n",
            "\n",
            "epoch:  542\n",
            "Train loss: 0.011415\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073173 \n",
            "\n",
            "epoch:  543\n",
            "Train loss: 0.011406\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073132 \n",
            "\n",
            "epoch:  544\n",
            "Train loss: 0.011359\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073080 \n",
            "\n",
            "epoch:  545\n",
            "Train loss: 0.011338\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.073033 \n",
            "\n",
            "epoch:  546\n",
            "Train loss: 0.011307\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072989 \n",
            "\n",
            "epoch:  547\n",
            "Train loss: 0.011294\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072944 \n",
            "\n",
            "epoch:  548\n",
            "Train loss: 0.011250\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072894 \n",
            "\n",
            "epoch:  549\n",
            "Train loss: 0.011230\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072850 \n",
            "\n",
            "epoch:  550\n",
            "Train loss: 0.011207\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072803 \n",
            "\n",
            "epoch:  551\n",
            "Train loss: 0.011190\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072762 \n",
            "\n",
            "epoch:  552\n",
            "Train loss: 0.011156\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072712 \n",
            "\n",
            "epoch:  553\n",
            "Train loss: 0.011130\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072673 \n",
            "\n",
            "epoch:  554\n",
            "Train loss: 0.011111\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072620 \n",
            "\n",
            "epoch:  555\n",
            "Train loss: 0.011097\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072578 \n",
            "\n",
            "epoch:  556\n",
            "Train loss: 0.011054\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072533 \n",
            "\n",
            "epoch:  557\n",
            "Train loss: 0.011028\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072487 \n",
            "\n",
            "epoch:  558\n",
            "Train loss: 0.011019\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072444 \n",
            "\n",
            "epoch:  559\n",
            "Train loss: 0.010988\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072397 \n",
            "\n",
            "epoch:  560\n",
            "Train loss: 0.010965\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072354 \n",
            "\n",
            "epoch:  561\n",
            "Train loss: 0.010940\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072310 \n",
            "\n",
            "epoch:  562\n",
            "Train loss: 0.010923\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072265 \n",
            "\n",
            "epoch:  563\n",
            "Train loss: 0.010908\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072225 \n",
            "\n",
            "epoch:  564\n",
            "Train loss: 0.010881\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072179 \n",
            "\n",
            "epoch:  565\n",
            "Train loss: 0.010863\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072140 \n",
            "\n",
            "epoch:  566\n",
            "Train loss: 0.010840\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072097 \n",
            "\n",
            "epoch:  567\n",
            "Train loss: 0.010811\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.072054 \n",
            "\n",
            "epoch:  568\n",
            "Train loss: 0.010797\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.072019 \n",
            "\n",
            "epoch:  569\n",
            "Train loss: 0.010770\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071977 \n",
            "\n",
            "epoch:  570\n",
            "Train loss: 0.010755\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071940 \n",
            "\n",
            "epoch:  571\n",
            "Train loss: 0.010718\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071899 \n",
            "\n",
            "epoch:  572\n",
            "Train loss: 0.010695\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071859 \n",
            "\n",
            "epoch:  573\n",
            "Train loss: 0.010665\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071815 \n",
            "\n",
            "epoch:  574\n",
            "Train loss: 0.010638\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071773 \n",
            "\n",
            "epoch:  575\n",
            "Train loss: 0.010622\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071734 \n",
            "\n",
            "epoch:  576\n",
            "Train loss: 0.010585\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071695 \n",
            "\n",
            "epoch:  577\n",
            "Train loss: 0.010562\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071658 \n",
            "\n",
            "epoch:  578\n",
            "Train loss: 0.010523\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071616 \n",
            "\n",
            "epoch:  579\n",
            "Train loss: 0.010501\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071575 \n",
            "\n",
            "epoch:  580\n",
            "Train loss: 0.010469\n",
            "Test loss: \n",
            " Accuracy: 97.8%, Avg loss: 0.071542 \n",
            "\n",
            "epoch:  581\n",
            "Train loss: 0.010446\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071507 \n",
            "\n",
            "epoch:  582\n",
            "Train loss: 0.010417\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071466 \n",
            "\n",
            "epoch:  583\n",
            "Train loss: 0.010384\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071431 \n",
            "\n",
            "epoch:  584\n",
            "Train loss: 0.010347\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071395 \n",
            "\n",
            "epoch:  585\n",
            "Train loss: 0.010325\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071363 \n",
            "\n",
            "epoch:  586\n",
            "Train loss: 0.010307\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071330 \n",
            "\n",
            "epoch:  587\n",
            "Train loss: 0.010265\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071291 \n",
            "\n",
            "epoch:  588\n",
            "Train loss: 0.010245\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071255 \n",
            "\n",
            "epoch:  589\n",
            "Train loss: 0.010221\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071217 \n",
            "\n",
            "epoch:  590\n",
            "Train loss: 0.010180\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071182 \n",
            "\n",
            "epoch:  591\n",
            "Train loss: 0.010161\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071149 \n",
            "\n",
            "epoch:  592\n",
            "Train loss: 0.010131\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071113 \n",
            "\n",
            "epoch:  593\n",
            "Train loss: 0.010103\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071078 \n",
            "\n",
            "epoch:  594\n",
            "Train loss: 0.010072\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071040 \n",
            "\n",
            "epoch:  595\n",
            "Train loss: 0.010046\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.071009 \n",
            "\n",
            "epoch:  596\n",
            "Train loss: 0.010021\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070976 \n",
            "\n",
            "epoch:  597\n",
            "Train loss: 0.010000\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070942 \n",
            "\n",
            "epoch:  598\n",
            "Train loss: 0.009965\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070905 \n",
            "\n",
            "epoch:  599\n",
            "Train loss: 0.009938\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070873 \n",
            "\n",
            "epoch:  600\n",
            "Train loss: 0.009917\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070839 \n",
            "\n",
            "epoch:  601\n",
            "Train loss: 0.009886\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070808 \n",
            "\n",
            "epoch:  602\n",
            "Train loss: 0.009868\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070771 \n",
            "\n",
            "epoch:  603\n",
            "Train loss: 0.009850\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070742 \n",
            "\n",
            "epoch:  604\n",
            "Train loss: 0.009814\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070706 \n",
            "\n",
            "epoch:  605\n",
            "Train loss: 0.009782\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070673 \n",
            "\n",
            "epoch:  606\n",
            "Train loss: 0.009760\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070641 \n",
            "\n",
            "epoch:  607\n",
            "Train loss: 0.009733\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070610 \n",
            "\n",
            "epoch:  608\n",
            "Train loss: 0.009712\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070576 \n",
            "\n",
            "epoch:  609\n",
            "Train loss: 0.009683\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070542 \n",
            "\n",
            "epoch:  610\n",
            "Train loss: 0.009668\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070515 \n",
            "\n",
            "epoch:  611\n",
            "Train loss: 0.009642\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070481 \n",
            "\n",
            "epoch:  612\n",
            "Train loss: 0.009624\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070452 \n",
            "\n",
            "epoch:  613\n",
            "Train loss: 0.009594\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070426 \n",
            "\n",
            "epoch:  614\n",
            "Train loss: 0.009565\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070390 \n",
            "\n",
            "epoch:  615\n",
            "Train loss: 0.009538\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070366 \n",
            "\n",
            "epoch:  616\n",
            "Train loss: 0.009526\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070336 \n",
            "\n",
            "epoch:  617\n",
            "Train loss: 0.009493\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070302 \n",
            "\n",
            "epoch:  618\n",
            "Train loss: 0.009466\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070272 \n",
            "\n",
            "epoch:  619\n",
            "Train loss: 0.009441\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070246 \n",
            "\n",
            "epoch:  620\n",
            "Train loss: 0.009423\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070212 \n",
            "\n",
            "epoch:  621\n",
            "Train loss: 0.009393\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070185 \n",
            "\n",
            "epoch:  622\n",
            "Train loss: 0.009374\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070154 \n",
            "\n",
            "epoch:  623\n",
            "Train loss: 0.009347\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070126 \n",
            "\n",
            "epoch:  624\n",
            "Train loss: 0.009326\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070105 \n",
            "\n",
            "epoch:  625\n",
            "Train loss: 0.009316\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070076 \n",
            "\n",
            "epoch:  626\n",
            "Train loss: 0.009287\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070048 \n",
            "\n",
            "epoch:  627\n",
            "Train loss: 0.009264\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.070019 \n",
            "\n",
            "epoch:  628\n",
            "Train loss: 0.009247\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069995 \n",
            "\n",
            "epoch:  629\n",
            "Train loss: 0.009225\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069965 \n",
            "\n",
            "epoch:  630\n",
            "Train loss: 0.009205\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069941 \n",
            "\n",
            "epoch:  631\n",
            "Train loss: 0.009174\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069911 \n",
            "\n",
            "epoch:  632\n",
            "Train loss: 0.009157\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069883 \n",
            "\n",
            "epoch:  633\n",
            "Train loss: 0.009136\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069861 \n",
            "\n",
            "epoch:  634\n",
            "Train loss: 0.009119\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069832 \n",
            "\n",
            "epoch:  635\n",
            "Train loss: 0.009095\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069806 \n",
            "\n",
            "epoch:  636\n",
            "Train loss: 0.009079\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069782 \n",
            "\n",
            "epoch:  637\n",
            "Train loss: 0.009050\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069758 \n",
            "\n",
            "epoch:  638\n",
            "Train loss: 0.009026\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069731 \n",
            "\n",
            "epoch:  639\n",
            "Train loss: 0.009000\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069705 \n",
            "\n",
            "epoch:  640\n",
            "Train loss: 0.008985\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069682 \n",
            "\n",
            "epoch:  641\n",
            "Train loss: 0.008970\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069655 \n",
            "\n",
            "epoch:  642\n",
            "Train loss: 0.008946\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069635 \n",
            "\n",
            "epoch:  643\n",
            "Train loss: 0.008923\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069609 \n",
            "\n",
            "epoch:  644\n",
            "Train loss: 0.008904\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069580 \n",
            "\n",
            "epoch:  645\n",
            "Train loss: 0.008876\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069558 \n",
            "\n",
            "epoch:  646\n",
            "Train loss: 0.008857\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069534 \n",
            "\n",
            "epoch:  647\n",
            "Train loss: 0.008841\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069511 \n",
            "\n",
            "epoch:  648\n",
            "Train loss: 0.008824\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069487 \n",
            "\n",
            "epoch:  649\n",
            "Train loss: 0.008800\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069465 \n",
            "\n",
            "epoch:  650\n",
            "Train loss: 0.008769\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069437 \n",
            "\n",
            "epoch:  651\n",
            "Train loss: 0.008752\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069415 \n",
            "\n",
            "epoch:  652\n",
            "Train loss: 0.008724\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069388 \n",
            "\n",
            "epoch:  653\n",
            "Train loss: 0.008711\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069367 \n",
            "\n",
            "epoch:  654\n",
            "Train loss: 0.008687\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069344 \n",
            "\n",
            "epoch:  655\n",
            "Train loss: 0.008675\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069316 \n",
            "\n",
            "epoch:  656\n",
            "Train loss: 0.008651\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069291 \n",
            "\n",
            "epoch:  657\n",
            "Train loss: 0.008634\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069268 \n",
            "\n",
            "epoch:  658\n",
            "Train loss: 0.008615\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069242 \n",
            "\n",
            "epoch:  659\n",
            "Train loss: 0.008599\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069223 \n",
            "\n",
            "epoch:  660\n",
            "Train loss: 0.008571\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069199 \n",
            "\n",
            "epoch:  661\n",
            "Train loss: 0.008553\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069176 \n",
            "\n",
            "epoch:  662\n",
            "Train loss: 0.008533\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069153 \n",
            "\n",
            "epoch:  663\n",
            "Train loss: 0.008515\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069132 \n",
            "\n",
            "epoch:  664\n",
            "Train loss: 0.008486\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069104 \n",
            "\n",
            "epoch:  665\n",
            "Train loss: 0.008471\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069083 \n",
            "\n",
            "epoch:  666\n",
            "Train loss: 0.008450\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069061 \n",
            "\n",
            "epoch:  667\n",
            "Train loss: 0.008422\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069036 \n",
            "\n",
            "epoch:  668\n",
            "Train loss: 0.008405\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.069015 \n",
            "\n",
            "epoch:  669\n",
            "Train loss: 0.008382\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068995 \n",
            "\n",
            "epoch:  670\n",
            "Train loss: 0.008368\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068973 \n",
            "\n",
            "epoch:  671\n",
            "Train loss: 0.008353\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068948 \n",
            "\n",
            "epoch:  672\n",
            "Train loss: 0.008321\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068931 \n",
            "\n",
            "epoch:  673\n",
            "Train loss: 0.008301\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068904 \n",
            "\n",
            "epoch:  674\n",
            "Train loss: 0.008287\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068884 \n",
            "\n",
            "epoch:  675\n",
            "Train loss: 0.008257\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068863 \n",
            "\n",
            "epoch:  676\n",
            "Train loss: 0.008240\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068840 \n",
            "\n",
            "epoch:  677\n",
            "Train loss: 0.008215\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068817 \n",
            "\n",
            "epoch:  678\n",
            "Train loss: 0.008184\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068794 \n",
            "\n",
            "epoch:  679\n",
            "Train loss: 0.008178\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068780 \n",
            "\n",
            "epoch:  680\n",
            "Train loss: 0.008145\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068755 \n",
            "\n",
            "epoch:  681\n",
            "Train loss: 0.008132\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068732 \n",
            "\n",
            "epoch:  682\n",
            "Train loss: 0.008102\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068715 \n",
            "\n",
            "epoch:  683\n",
            "Train loss: 0.008086\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068690 \n",
            "\n",
            "epoch:  684\n",
            "Train loss: 0.008057\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068676 \n",
            "\n",
            "epoch:  685\n",
            "Train loss: 0.008033\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068656 \n",
            "\n",
            "epoch:  686\n",
            "Train loss: 0.008018\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068636 \n",
            "\n",
            "epoch:  687\n",
            "Train loss: 0.008000\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068615 \n",
            "\n",
            "epoch:  688\n",
            "Train loss: 0.007975\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068598 \n",
            "\n",
            "epoch:  689\n",
            "Train loss: 0.007961\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068575 \n",
            "\n",
            "epoch:  690\n",
            "Train loss: 0.007933\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068554 \n",
            "\n",
            "epoch:  691\n",
            "Train loss: 0.007919\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068540 \n",
            "\n",
            "epoch:  692\n",
            "Train loss: 0.007896\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068518 \n",
            "\n",
            "epoch:  693\n",
            "Train loss: 0.007881\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068501 \n",
            "\n",
            "epoch:  694\n",
            "Train loss: 0.007859\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068480 \n",
            "\n",
            "epoch:  695\n",
            "Train loss: 0.007829\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068460 \n",
            "\n",
            "epoch:  696\n",
            "Train loss: 0.007817\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068451 \n",
            "\n",
            "epoch:  697\n",
            "Train loss: 0.007787\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068431 \n",
            "\n",
            "epoch:  698\n",
            "Train loss: 0.007769\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068414 \n",
            "\n",
            "epoch:  699\n",
            "Train loss: 0.007756\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068396 \n",
            "\n",
            "epoch:  700\n",
            "Train loss: 0.007726\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068378 \n",
            "\n",
            "epoch:  701\n",
            "Train loss: 0.007694\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068357 \n",
            "\n",
            "epoch:  702\n",
            "Train loss: 0.007678\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068345 \n",
            "\n",
            "epoch:  703\n",
            "Train loss: 0.007660\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068328 \n",
            "\n",
            "epoch:  704\n",
            "Train loss: 0.007636\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068312 \n",
            "\n",
            "epoch:  705\n",
            "Train loss: 0.007618\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068294 \n",
            "\n",
            "epoch:  706\n",
            "Train loss: 0.007598\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068278 \n",
            "\n",
            "epoch:  707\n",
            "Train loss: 0.007571\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068265 \n",
            "\n",
            "epoch:  708\n",
            "Train loss: 0.007543\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068246 \n",
            "\n",
            "epoch:  709\n",
            "Train loss: 0.007531\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068234 \n",
            "\n",
            "epoch:  710\n",
            "Train loss: 0.007503\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068219 \n",
            "\n",
            "epoch:  711\n",
            "Train loss: 0.007484\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068201 \n",
            "\n",
            "epoch:  712\n",
            "Train loss: 0.007457\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068189 \n",
            "\n",
            "epoch:  713\n",
            "Train loss: 0.007429\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068173 \n",
            "\n",
            "epoch:  714\n",
            "Train loss: 0.007405\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068159 \n",
            "\n",
            "epoch:  715\n",
            "Train loss: 0.007388\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068143 \n",
            "\n",
            "epoch:  716\n",
            "Train loss: 0.007356\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068131 \n",
            "\n",
            "epoch:  717\n",
            "Train loss: 0.007328\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068115 \n",
            "\n",
            "epoch:  718\n",
            "Train loss: 0.007309\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068100 \n",
            "\n",
            "epoch:  719\n",
            "Train loss: 0.007296\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068091 \n",
            "\n",
            "epoch:  720\n",
            "Train loss: 0.007255\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068072 \n",
            "\n",
            "epoch:  721\n",
            "Train loss: 0.007248\n",
            "Test loss: \n",
            " Accuracy: 97.9%, Avg loss: 0.068061 \n",
            "\n",
            "epoch:  722\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8bfaf5a27ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_2NN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mtest_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_2NN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-8bfaf5a27ed6>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define how we train our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# fetch xb (a batch of images) and yb (a batch of labels) from train_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# compute the loss value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}