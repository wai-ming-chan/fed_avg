{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wai-ming-chan/fed_avg/blob/main/%5BQua%5D_Federated_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNBKOILTIJDs"
      },
      "source": [
        "# Communication-Efficient Learning of Deep Networks from Decentralized Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark - Federated Average Learning [McMahan17]\n"
      ],
      "metadata": {
        "id": "IGgawXwrwm96"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmmGuEPIiF-"
      },
      "source": [
        "## Helper files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHxXIQLJAFzo"
      },
      "source": [
        "# import global dependencies\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import random\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlFXcKdIBYBA"
      },
      "source": [
        "from torch import autograd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "class DatasetSplit(Dataset):\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = list(idxs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, label = self.dataset[self.idxs[item]]\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class LocalUpdate(object):\n",
        "    def __init__(self, args, dataset=None, idxs=None):\n",
        "        self.args = args\n",
        "        self.loss_func = nn.CrossEntropyLoss()\n",
        "        self.selected_clients = []\n",
        "        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.args.local_bs, shuffle=True)\n",
        "\n",
        "    def train(self, net):\n",
        "        net.train()\n",
        "        # train and update\n",
        "        optimizer = torch.optim.SGD(net.parameters(), lr=self.args.lr, momentum=0.5)\n",
        "\n",
        "        epoch_loss = []\n",
        "        for iter in range(self.args.local_ep):\n",
        "            batch_loss = []\n",
        "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
        "                images, labels = images.to(self.args.device), labels.to(self.args.device)\n",
        "                net.zero_grad()\n",
        "                log_probs = net(images)\n",
        "                loss = self.loss_func(log_probs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                if self.args.verbose and batch_idx % 10 == 0:\n",
        "                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n",
        "                               100. * batch_idx / len(self.ldr_train), loss.item()))\n",
        "                batch_loss.append(loss.item())\n",
        "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
        "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL_C_dGoB0ch"
      },
      "source": [
        "class CNNMnist(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNMnist, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, args.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Server function to calculate the average of model parameters received from the current sets of clients\n"
      ],
      "metadata": {
        "id": "V_iAIlWlw9Uo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYwXOWaUCD0M"
      },
      "source": [
        "import copy\n",
        "\n",
        "# Server function to calculate the average of model parameters received from the current sets of clients\n",
        "def FedAvg(w, clients):\n",
        "    w_avg = copy.deepcopy(w[0])\n",
        "    for k in w_avg.keys():\n",
        "        for i in range(1, len(w)):\n",
        "            tens = torch.mul(w[i][k], clients[i])\n",
        "            w_avg[k] += tens\n",
        "        w_avg[k] = torch.div(w_avg[k], sum(clients))\n",
        "    return w_avg"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOpNFnCQCSBu"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def test_img(net_g, datatest, args):\n",
        "    net_g.eval()\n",
        "    # testing\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    data_loader = DataLoader(datatest, batch_size=args.bs)\n",
        "    l = len(data_loader)\n",
        "    for idx, (data, target) in enumerate(data_loader):\n",
        "        if args.gpu != -1:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        log_probs = net_g(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
        "        # get the index of the max log-probability\n",
        "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
        "        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
        "\n",
        "    test_loss /= len(data_loader.dataset)\n",
        "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
        "    if args.verbose:\n",
        "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
        "    return accuracy, test_loss\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrkWmj6zAXga"
      },
      "source": [
        "# Function to separate training data to users in an IID manner\n",
        "def mnist_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample I.I.D. client data from MNIST dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return: dict of image index\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset)/num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(\n",
        "            all_idxs,\n",
        "            random.randint(1,num_items),\n",
        "            replace=False))\n",
        "        print(len(dict_users[i]))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "    return dict_users"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njAouhS1Ivj7"
      },
      "source": [
        "## Data in Uniform Distribution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLnBHMzN_KAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f306b615-0e43-422a-99a0-d42f9c8c9cdd"
      },
      "source": [
        "# parse args\n",
        "class args:\n",
        "    gpu = -1 # <- -1 if no GPU is available\n",
        "    device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
        "    num_channels = 1\n",
        "    num_users = 100\n",
        "    num_classes = 10\n",
        "    frac = 0.1\n",
        "    lr = 0.1\n",
        "    verbose = 0\n",
        "    bs = 128\n",
        "    epochs = 100\n",
        "    \n",
        "    iid = True        # < -This Value needs to be changed\n",
        "    local_ep = 20     # <- This Value needs to be changed\n",
        "    local_bs = 10     # <- This Value needs to be changed\n",
        "\n",
        "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
        "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
        "# sample users\n",
        "if args.iid:\n",
        "    dict_users = mnist_iid(dataset_train, args.num_users)\n",
        "else:\n",
        "    dict_users = mnist_noniid(dataset_train, args.num_users)\n",
        "\n",
        "img_size = dataset_train[0][0].shape\n",
        "\n",
        "# build model\n",
        "\n",
        "net_glob = CNNMnist(args=args).to(args.device)\n",
        "print(net_glob)\n",
        "net_glob.train()\n",
        "\n",
        "# copy weights\n",
        "w_glob = net_glob.state_dict()\n",
        "\n",
        "# training\n",
        "loss_train = []\n",
        "cv_loss, cv_acc = [], []\n",
        "val_loss_pre, counter = 0, 0\n",
        "net_best = None\n",
        "best_loss = None\n",
        "val_acc_list, net_list = [], []"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199\n",
            "295\n",
            "569\n",
            "494\n",
            "140\n",
            "335\n",
            "568\n",
            "372\n",
            "356\n",
            "499\n",
            "277\n",
            "560\n",
            "127\n",
            "1\n",
            "344\n",
            "187\n",
            "266\n",
            "531\n",
            "103\n",
            "268\n",
            "37\n",
            "587\n",
            "168\n",
            "197\n",
            "285\n",
            "406\n",
            "169\n",
            "119\n",
            "390\n",
            "152\n",
            "472\n",
            "395\n",
            "42\n",
            "135\n",
            "394\n",
            "586\n",
            "351\n",
            "383\n",
            "396\n",
            "408\n",
            "388\n",
            "548\n",
            "291\n",
            "541\n",
            "574\n",
            "587\n",
            "574\n",
            "433\n",
            "120\n",
            "119\n",
            "479\n",
            "55\n",
            "137\n",
            "76\n",
            "582\n",
            "46\n",
            "129\n",
            "422\n",
            "56\n",
            "9\n",
            "212\n",
            "194\n",
            "181\n",
            "380\n",
            "324\n",
            "215\n",
            "258\n",
            "108\n",
            "538\n",
            "441\n",
            "87\n",
            "63\n",
            "515\n",
            "599\n",
            "63\n",
            "71\n",
            "460\n",
            "593\n",
            "8\n",
            "260\n",
            "24\n",
            "110\n",
            "400\n",
            "405\n",
            "421\n",
            "4\n",
            "435\n",
            "22\n",
            "597\n",
            "46\n",
            "168\n",
            "359\n",
            "245\n",
            "25\n",
            "236\n",
            "288\n",
            "501\n",
            "340\n",
            "490\n",
            "151\n",
            "CNNMnist(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC7esCT0MJKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc51d69-6655-43ec-debc-1d7673714802"
      },
      "source": [
        "for iter in range(args.epochs):\n",
        "\n",
        "    # w_locals, loss_locals = [], []\n",
        "    w_locals, loss_locals, num_items = [], [], []\n",
        "    m = max(int(args.frac * args.num_users), 1)\n",
        "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
        "    for idx in idxs_users:\n",
        "      num_items.append(len(dict_users[idx]))  \n",
        "      local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
        "      w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
        "      w_locals.append(copy.deepcopy(w))\n",
        "      loss_locals.append(copy.deepcopy(loss))\n",
        "    # update global weights\n",
        "    w_glob = FedAvg(w_locals, num_items)\n",
        "\n",
        "    # copy weight to net_glob\n",
        "    net_glob.load_state_dict(w_glob)\n",
        "\n",
        "    # print loss\n",
        "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
        "    \n",
        "    loss_train.append(loss_avg)\n",
        "    \n",
        "    # Evaluate score\n",
        "    net_glob.eval()\n",
        "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n",
        "\n",
        "# testing\n",
        "net_glob.eval()\n",
        "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
        "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
        "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round   0, Average loss 1.841, Accuracy 11.350\n",
            "Round   1, Average loss 1.205, Accuracy 57.870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0uGPKP3I9dl"
      },
      "source": [
        "## Uneven Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwHcoyrVLTHU"
      },
      "source": [
        "for iter in range(args.epochs):\n",
        "\n",
        "    # w_locals, loss_locals = [], []\n",
        "    w_locals, loss_locals, num_items = [], [], []\n",
        "    m = max(int(args.frac * args.num_users), 1)\n",
        "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
        "    for idx in idxs_users:\n",
        "      num_items.append(len(dict_users[idx]))\n",
        "      local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
        "      w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
        "      w_locals.append(copy.deepcopy(w))\n",
        "      loss_locals.append(copy.deepcopy(loss))\n",
        "    # update global weights\n",
        "    w_glob = FedAvg(w_locals, num_items)\n",
        "\n",
        "    # copy weight to net_glob\n",
        "    net_glob.load_state_dict(w_glob)\n",
        "\n",
        "    # print loss\n",
        "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
        "    \n",
        "    loss_train.append(loss_avg)\n",
        "    \n",
        "    # Evaluate score\n",
        "    net_glob.eval()\n",
        "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n",
        "\n",
        "# testing\n",
        "net_glob.eval()\n",
        "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
        "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
        "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsr9RS1cJDC4"
      },
      "source": [
        "## Uneven Distribution with Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kiwPkSZ_4Ba"
      },
      "source": [
        "for iter in range(args.epochs):\n",
        "\n",
        "    w_locals, loss_locals, num_items = [], [], []\n",
        "    m = max(int(args.frac * args.num_users), 1)\n",
        "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
        "    for idx in idxs_users:\n",
        "        num_items.append(len(dict_users[idx]))\n",
        "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
        "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
        "        w_locals.append(copy.deepcopy(w))\n",
        "        loss_locals.append(copy.deepcopy(loss))\n",
        "    # update global weights\n",
        "    w_glob = FedAvg(w_locals, num_items)\n",
        "\n",
        "    # copy weight to net_glob\n",
        "    net_glob.load_state_dict(w_glob)\n",
        "\n",
        "    # print loss\n",
        "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
        "    \n",
        "    loss_train.append(loss_avg)\n",
        "    \n",
        "    # Evaluate score\n",
        "    net_glob.eval()\n",
        "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n",
        "\n",
        "# testing\n",
        "net_glob.eval()\n",
        "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
        "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
        "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E19mxNYJRcN"
      },
      "source": [
        "## Adding Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC9ND0K_YU5x"
      },
      "source": [
        "for iter in range(args.epochs):\n",
        "\n",
        "    w_locals, loss_locals = [], []\n",
        "    m = max(int(args.frac * args.num_users), 1)\n",
        "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
        "    for idx in idxs_users:\n",
        "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
        "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
        "        \n",
        "        # Add noise to layers\n",
        "        sigma_squared = 0.3\n",
        "        for layer in w:\n",
        "            x = np.random.normal(0,sigma_squared,w[layer].size())\n",
        "            x = np.reshape(x,w[layer].size())\n",
        "            x = torch.from_numpy(x)\n",
        "            w[layer] = w[layer]+x.cuda()\n",
        "\n",
        "        w_locals.append(copy.deepcopy(w))\n",
        "        loss_locals.append(copy.deepcopy(loss))\n",
        "    # update global weights\n",
        "    w_glob = FedAvg(w_locals)\n",
        "\n",
        "    # copy weight to net_glob\n",
        "    net_glob.load_state_dict(w_glob)\n",
        "\n",
        "    # print loss\n",
        "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
        "    \n",
        "    loss_train.append(loss_avg)\n",
        "    \n",
        "    # Evaluate score\n",
        "    net_glob.eval()\n",
        "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n",
        "\n",
        "# testing\n",
        "net_glob.eval()\n",
        "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
        "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
        "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}